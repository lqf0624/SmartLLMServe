#!/usr/bin/env python3
"""
å¯¹æ¯”baseline_tsv.csvå’Œexample_run.csvçš„æ¨¡æ‹Ÿç»“æœ
éªŒè¯LLMServingSimæ¨¡æ‹Ÿçš„åˆç†æ€§
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path

def load_and_compare_data():
    """åŠ è½½å¹¶å¯¹æ¯”ä¸¤ä¸ªæ•°æ®æ–‡ä»¶"""
    print("ğŸ” åŠ è½½å¯¹æ¯”æ•°æ®...")

    # åŠ è½½æ•°æ®
    baseline_df = pd.read_csv('output/baseline_tsv.csv')
    example_df = pd.read_csv('output/example_run.csv')

    print(f"baseline_tsv.csv: {len(baseline_df)} ä¸ªè¯·æ±‚")
    print(f"example_run.csv: {len(example_df)} ä¸ªè¯·æ±‚")

    return baseline_df, example_df

def analyze_time_units(df, name):
    """åˆ†ææ—¶é—´å•ä½å’Œæ•°æ®èŒƒå›´"""
    print(f"\nğŸ“Š {name} æ•°æ®åˆ†æ:")

    # è½¬æ¢ä¸ºåˆç†å•ä½æ˜¾ç¤º
    df_copy = df.copy()
    df_copy['latency_ms'] = df_copy['latency'] / 1e6
    df_copy['ttft_ms'] = df_copy['TTFT'] / 1e6
    df_copy['tpot_ms'] = df_copy['TPOT'] / 1e6
    df_copy['queue_delay_ms'] = df_copy['queuing_delay'] / 1e6

    print(f"  â€¢ å»¶è¿ŸèŒƒå›´: {df_copy['latency_ms'].min():.1f} - {df_copy['latency_ms'].max():.1f} ms")
    print(f"  â€¢ TTFTèŒƒå›´: {df_copy['ttft_ms'].min():.1f} - {df_copy['ttft_ms'].max():.1f} ms")
    print(f"  â€¢ TPOTèŒƒå›´: {df_copy['tpot_ms'].min():.1f} - {df_copy['tpot_ms'].max():.1f} ms")
    print(f"  â€¢ é˜Ÿåˆ—å»¶è¿ŸèŒƒå›´: {df_copy['queue_delay_ms'].min():.1f} - {df_copy['queue_delay_ms'].max():.1f} ms")

    return df_copy

def compare_performance_metrics(baseline_df, example_df):
    """å¯¹æ¯”æ€§èƒ½æŒ‡æ ‡"""
    print("\nğŸ“ˆ æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”:")

    # è®¡ç®—å…³é”®æŒ‡æ ‡
    def calculate_metrics(df, name):
        total_time = (df['end_time'].max() - df['arrival'].min()) / 1e9
        total_output = df['output'].sum()
        throughput = total_output / total_time if total_time > 0 else 0

        metrics = {
            'name': name,
            'requests': len(df),
            'total_time_s': total_time,
            'avg_latency_ms': df['latency'].mean() / 1e6,
            'avg_ttft_ms': df['TTFT'].mean() / 1e6,
            'avg_tpot_ms': df['TPOT'].mean() / 1e6,
            'throughput_tps': throughput,
            'latency_std_ms': df['latency'].std() / 1e6,
            'avg_queue_delay_ms': df['queuing_delay'].mean() / 1e6
        }
        return metrics

    baseline_metrics = calculate_metrics(baseline_df, "Baseline (20è¯·æ±‚)")
    example_metrics = calculate_metrics(example_df, "Example (12è¯·æ±‚)")

    # æ‰“å°å¯¹æ¯”è¡¨æ ¼
    print("\n| æŒ‡æ ‡ | Baseline | Example | å·®å¼‚ |")
    print("|------|----------|---------|------|")

    comparisons = [
        ('è¯·æ±‚æ•°', f"{baseline_metrics['requests']}", f"{example_metrics['requests']}", f"{baseline_metrics['requests'] - example_metrics['requests']}"),
        ('æ€»æ—¶é—´(s)', f"{baseline_metrics['total_time_s']:.2f}", f"{example_metrics['total_time_s']:.2f}", f"{baseline_metrics['total_time_s'] - example_metrics['total_time_s']:.2f}"),
        ('å¹³å‡å»¶è¿Ÿ(ms)', f"{baseline_metrics['avg_latency_ms']:.1f}", f"{example_metrics['avg_latency_ms']:.1f}", f"{baseline_metrics['avg_latency_ms'] - example_metrics['avg_latency_ms']:.1f}"),
        ('å¹³å‡TTFT(ms)', f"{baseline_metrics['avg_ttft_ms']:.1f}", f"{example_metrics['avg_ttft_ms']:.1f}", f"{baseline_metrics['avg_ttft_ms'] - example_metrics['avg_ttft_ms']:.1f}"),
        ('å¹³å‡TPOT(ms)', f"{baseline_metrics['avg_tpot_ms']:.1f}", f"{example_metrics['avg_tpot_ms']:.1f}", f"{baseline_metrics['avg_tpot_ms'] - example_metrics['avg_tpot_ms']:.1f}"),
        ('ååé‡(tok/s)', f"{baseline_metrics['throughput_tps']:.1f}", f"{example_metrics['throughput_tps']:.1f}", f"{baseline_metrics['throughput_tps'] - example_metrics['throughput_tps']:.1f}"),
        ('å»¶è¿Ÿæ ‡å‡†å·®(ms)', f"{baseline_metrics['latency_std_ms']:.1f}", f"{example_metrics['latency_std_ms']:.1f}", f"{baseline_metrics['latency_std_ms'] - example_metrics['latency_std_ms']:.1f}"),
        ('é˜Ÿåˆ—å»¶è¿Ÿ(ms)', f"{baseline_metrics['avg_queue_delay_ms']:.1f}", f"{example_metrics['avg_queue_delay_ms']:.1f}", f"{baseline_metrics['avg_queue_delay_ms'] - example_metrics['avg_queue_delay_ms']:.1f}")
    ]

    for metric, baseline_val, example_val, diff in comparisons:
        print(f"| {metric} | {baseline_val} | {example_val} | {diff} |")

    return baseline_metrics, example_metrics

def analyze_simulation_reasonableness(baseline_metrics, example_metrics):
    """åˆ†ææ¨¡æ‹Ÿç»“æœçš„åˆç†æ€§"""
    print("\nğŸ¤” æ¨¡æ‹Ÿç»“æœåˆç†æ€§åˆ†æ:")

    # æ£€æŸ¥å…³é”®æŒ‡æ ‡çš„åˆç†æ€§
    checks = []

    # 1. å»¶è¿Ÿåˆç†æ€§æ£€æŸ¥
    baseline_latency = baseline_metrics['avg_latency_ms']
    example_latency = example_metrics['avg_latency_ms']

    if 10 < baseline_latency < 100000:  # 10ms to 100s
        checks.append("âœ… Baselineå»¶è¿Ÿåœ¨åˆç†èŒƒå›´å†…")
    else:
        checks.append(f"âš ï¸ Baselineå»¶è¿Ÿå¼‚å¸¸: {baseline_latency:.1f}ms")

    if 10 < example_latency < 100000:
        checks.append("âœ… Exampleå»¶è¿Ÿåœ¨åˆç†èŒƒå›´å†…")
    else:
        checks.append(f"âš ï¸ Exampleå»¶è¿Ÿå¼‚å¸¸: {example_latency:.1f}ms")

    # 2. TTFTåˆç†æ€§æ£€æŸ¥ (é€šå¸¸50-500ms)
    baseline_ttft = baseline_metrics['avg_ttft_ms']
    example_ttft = example_metrics['avg_ttft_ms']

    if 50 < baseline_ttft < 2000:
        checks.append("âœ… Baseline TTFTåœ¨åˆç†èŒƒå›´å†…")
    else:
        checks.append(f"âš ï¸ Baseline TTFTå¼‚å¸¸: {baseline_ttft:.1f}ms")

    if 50 < example_ttft < 2000:
        checks.append("âœ… Example TTFTåœ¨åˆç†èŒƒå›´å†…")
    else:
        checks.append(f"âš ï¸ Example TTFTå¼‚å¸¸: {example_ttft:.1f}ms")

    # 3. TPOTåˆç†æ€§æ£€æŸ¥ (é€šå¸¸10-100ms per token)
    baseline_tpot = baseline_metrics['avg_tpot_ms']
    example_tpot = example_metrics['avg_tpot_ms']

    if 10 < baseline_tpot < 500:
        checks.append("âœ… Baseline TPOTåœ¨åˆç†èŒƒå›´å†…")
    else:
        checks.append(f"âš ï¸ Baseline TPOTå¼‚å¸¸: {baseline_tpot:.1f}ms")

    if 10 < example_tpot < 500:
        checks.append("âœ… Example TPOTåœ¨åˆç†èŒƒå›´å†…")
    else:
        checks.append(f"âš ï¸ Example TPOTå¼‚å¸¸: {example_tpot:.1f}ms")

    # 4. ååé‡åˆç†æ€§æ£€æŸ¥
    baseline_throughput = baseline_metrics['throughput_tps']
    example_throughput = example_metrics['throughput_tps']

    if 10 < baseline_throughput < 1000:
        checks.append("âœ… Baselineååé‡åœ¨åˆç†èŒƒå›´å†…")
    else:
        checks.append(f"âš ï¸ Baselineååé‡å¼‚å¸¸: {baseline_throughput:.1f} tok/s")

    if 10 < example_throughput < 1000:
        checks.append("âœ… Exampleååé‡åœ¨åˆç†èŒƒå›´å†…")
    else:
        checks.append(f"âš ï¸ Exampleååé‡å¼‚å¸¸: {example_throughput:.1f} tok/s")

    for check in checks:
        print(f"  {check}")

    # 5. å¯¹æ¯”åˆ†æ
    print(f"\nğŸ“Š å¯¹æ¯”åˆ†æ:")
    latency_ratio = baseline_latency / example_latency if example_latency > 0 else float('inf')
    throughput_ratio = baseline_throughput / example_throughput if example_throughput > 0 else float('inf')

    print(f"  â€¢ Baselineå»¶è¿Ÿæ˜¯Exampleçš„ {latency_ratio:.1f} å€")
    print(f"  â€¢ Baselineååé‡æ˜¯Exampleçš„ {throughput_ratio:.1f} å€")

    if 0.5 < latency_ratio < 2.0:
        print("  âœ… å»¶è¿Ÿè¡¨ç°ç›¸å¯¹ä¸€è‡´")
    else:
        print("  âš ï¸ å»¶è¿Ÿè¡¨ç°å·®å¼‚è¾ƒå¤§")

    if 0.5 < throughput_ratio < 2.0:
        print("  âœ… ååé‡è¡¨ç°ç›¸å¯¹ä¸€è‡´")
    else:
        print("  âš ï¸ ååé‡è¡¨ç°å·®å¼‚è¾ƒå¤§")

def create_comparison_charts(baseline_df, example_df):
    """åˆ›å»ºå¯¹æ¯”å›¾è¡¨"""
    # è½¬æ¢å•ä½
    baseline_df['latency_ms'] = baseline_df['latency'] / 1e6
    example_df['latency_ms'] = example_df['latency'] / 1e6

    baseline_df['ttft_ms'] = baseline_df['TTFT'] / 1e6
    example_df['ttft_ms'] = example_df['TTFT'] / 1e6

    baseline_df['tpot_ms'] = baseline_df['TPOT'] / 1e6
    example_df['tpot_ms'] = example_df['TPOT'] / 1e6

    # åˆ›å»ºå¯¹æ¯”å›¾
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    fig.suptitle('LLMServingSim Simulation Results Comparison', fontsize=16, fontweight='bold')

    # 1. å»¶è¿Ÿåˆ†å¸ƒå¯¹æ¯”
    ax1 = axes[0, 0]
    ax1.hist(baseline_df['latency_ms'], bins=10, alpha=0.7, label='Baseline (20 req)', color='blue')
    ax1.hist(example_df['latency_ms'], bins=10, alpha=0.7, label='Example (12 req)', color='orange')
    ax1.set_xlabel('Latency (ms)')
    ax1.set_ylabel('Request Count')
    ax1.set_title('Latency Distribution Comparison')
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    # 2. TTFT vs TPOT æ•£ç‚¹å›¾å¯¹æ¯”
    ax2 = axes[0, 1]
    ax2.scatter(baseline_df['ttft_ms'], baseline_df['tpot_ms'], alpha=0.7, label='Baseline', color='blue')
    ax2.scatter(example_df['ttft_ms'], example_df['tpot_ms'], alpha=0.7, label='Example', color='orange')
    ax2.set_xlabel('TTFT (ms)')
    ax2.set_ylabel('TPOT (ms)')
    ax2.set_title('TTFT vs TPOT Comparison')
    ax2.legend()
    ax2.grid(True, alpha=0.3)

    # 3. æ—¶é—´åºåˆ—å¯¹æ¯”
    ax3 = axes[1, 0]
    baseline_time = (baseline_df['arrival'] - baseline_df['arrival'].min()) / 1e9
    example_time = (example_df['arrival'] - example_df['arrival'].min()) / 1e9

    ax3.plot(baseline_time, baseline_df['latency_ms'], 'o-', label='Baseline', alpha=0.7, color='blue')
    ax3.plot(example_time, example_df['latency_ms'], 'o-', label='Example', alpha=0.7, color='orange')
    ax3.set_xlabel('Time (s)')
    ax3.set_ylabel('Latency (ms)')
    ax3.set_title('Timeline Comparison')
    ax3.legend()
    ax3.grid(True, alpha=0.3)

    # 4. è¾“å…¥é•¿åº¦vså»¶è¿Ÿå¯¹æ¯”
    ax4 = axes[1, 1]
    ax4.scatter(baseline_df['input'], baseline_df['latency_ms'], alpha=0.7, label='Baseline', color='blue')
    ax4.scatter(example_df['input'], example_df['latency_ms'], alpha=0.7, label='Example', color='orange')
    ax4.set_xlabel('Input Length (tokens)')
    ax4.set_ylabel('Latency (ms)')
    ax4.set_title('Input Length vs Latency')
    ax4.legend()
    ax4.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig('output/simulation_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()

    print("ğŸ“Š å¯¹æ¯”å›¾è¡¨å·²ä¿å­˜: output/simulation_comparison.png")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” å¼€å§‹å¯¹æ¯”LLMServingSimæ¨¡æ‹Ÿç»“æœ...")

    # åŠ è½½æ•°æ®
    baseline_df, example_df = load_and_compare_data()

    # åˆ†ææ•°æ®
    baseline_converted = analyze_time_units(baseline_df, "baseline_tsv.csv")
    example_converted = analyze_time_units(example_df, "example_run.csv")

    # å¯¹æ¯”æ€§èƒ½æŒ‡æ ‡
    baseline_metrics, example_metrics = compare_performance_metrics(baseline_df, example_df)

    # åˆ†æåˆç†æ€§
    analyze_simulation_reasonableness(baseline_metrics, example_metrics)

    # åˆ›å»ºå¯¹æ¯”å›¾è¡¨
    create_comparison_charts(baseline_df, example_df)

    print("\nâœ… å¯¹æ¯”åˆ†æå®Œæˆï¼")
    print("\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    print("  â€¢ output/simulation_comparison.png")

if __name__ == "__main__":
    main()