#!/usr/bin/env python3
"""
LLMè°ƒåº¦å™¨å±€é™æ€§æ¼”ç¤ºè„šæœ¬
å±•ç¤ºå½“å‰ç³»ç»Ÿçš„å…³é”®é—®é¢˜å’Œæ”¹è¿›æœºä¼š
"""

import pandas as pd
import numpy as np
from pathlib import Path

def print_limitation_summary():
    """æ‰“å°å±€é™æ€§åˆ†ææ‘˜è¦"""
    print("="*80)
    print("ğŸ” LLMè°ƒåº¦å™¨å½“å‰ç³»ç»Ÿå±€é™æ€§åˆ†æç»“æœ")
    print("="*80)

    # åŠ è½½æ•°æ®
    try:
        full_df = pd.read_csv('output/baseline_tsv_full.csv')
        partial_df = pd.read_csv('output/baseline_tsv.csv')

        print("\nğŸ“Š **æ ¸å¿ƒæ€§èƒ½æŒ‡æ ‡å¯¹æ¯”:**")
        print(f"å®Œæ•´æ•°æ®é›† (100ä¸ªè¯·æ±‚):")
        print(f"  â€¢ å¹³å‡å»¶è¿Ÿ: {full_df['latency'].mean()/1e6:.2f}s")
        print(f"  â€¢ å¹³å‡TTFT: {full_df['TTFT'].mean()/1e6:.2f}s")
        print(f"  â€¢ å¹³å‡TPOT: {full_df['TPOT'].mean()/1e6:.2f}s")
        print(f"  â€¢ æ€»ä½“ååé‡: {full_df['output'].sum()/(full_df['end_time'].max()/1e6):.2f} tokens/s")
        print(f"  â€¢ å»¶è¿Ÿæ ‡å‡†å·®: {full_df['latency'].std()/1e6:.2f}s")

        print(f"\néƒ¨åˆ†æ•°æ®é›† (20ä¸ªè¯·æ±‚):")
        print(f"  â€¢ å¹³å‡å»¶è¿Ÿ: {partial_df['latency'].mean()/1e6:.2f}s")
        print(f"  â€¢ æ€»ä½“ååé‡: {partial_df['output'].sum()/(partial_df['end_time'].max()/1e6):.2f} tokens/s")

        print("\nğŸš¨ **è¯†åˆ«çš„å…³é”®é—®é¢˜:**")

        # 1. å»¶è¿Ÿåˆ†å¸ƒé—®é¢˜
        latency_p95 = full_df['latency'].quantile(0.95) / 1e6
        latency_p5 = full_df['latency'].quantile(0.05) / 1e6
        print(f"1. **å»¶è¿Ÿåˆ†å¸ƒæä¸å‡è¡¡**")
        print(f"   â€¢ P95å»¶è¿Ÿ: {latency_p95:.2f}s, P5å»¶è¿Ÿ: {latency_p5:.2f}s")
        print(f"   â€¢ å»¶è¿Ÿå·®è·: {latency_p95/latency_p5:.1f}å€")

        # 2. é˜Ÿåˆ—å»¶è¿Ÿé—®é¢˜
        avg_queue_delay = full_df['queuing_delay'].mean() / 1e6
        max_queue_delay = full_df['queuing_delay'].max() / 1e6
        print(f"\n2. **é˜Ÿåˆ—ç®¡ç†æ•ˆç‡ä½ä¸‹**")
        print(f"   â€¢ å¹³å‡é˜Ÿåˆ—å»¶è¿Ÿ: {avg_queue_delay:.2f}s")
        print(f"   â€¢ æœ€å¤§é˜Ÿåˆ—å»¶è¿Ÿ: {max_queue_delay:.2f}s")
        print(f"   â€¢ é˜Ÿåˆ—å»¶è¿Ÿå æ¯”: {(avg_queue_delay/(full_df['latency'].mean()/1e6)*100):.1f}%")

        # 3. èµ„æºåˆ©ç”¨ç‡é—®é¢˜
        total_processing_time = full_df['latency'].sum() / 1e6
        total_output_tokens = full_df['output'].sum()
    theoretical_efficiency = (total_output_tokens * 0.001) / total_processing_time * 100  # å‡è®¾æ¯token 1mså¤„ç†æ—¶é—´
        print(f"\n3. **èµ„æºåˆ©ç”¨ç‡ä½æ•ˆ**")
        print(f"   â€¢ ç†è®ºå¤„ç†æ•ˆç‡: {theoretical_efficiency:.2f}%")
        print(f"   â€¢ å¤§é‡æ—¶é—´æµªè´¹åœ¨ç­‰å¾…å’Œè°ƒåº¦ä¸Š")

        # 4. è´Ÿè½½æ³¢åŠ¨æ€§é—®é¢˜
        arrival_intervals = np.diff(full_df['arrival'])
        cv = arrival_intervals.std() / arrival_intervals.mean()
        print(f"\n4. **ç¼ºä¹è´Ÿè½½é€‚åº”èƒ½åŠ›**")
        print(f"   â€¢ åˆ°è¾¾é—´éš”å˜å¼‚ç³»æ•°: {cv:.2f}")
        print(f"   â€¢ ç³»ç»Ÿæ— æ³•é€‚åº”è´Ÿè½½æ³¢åŠ¨")

        print("\nğŸ’¡ **SmartLLMServeæ”¹è¿›æœºä¼š:**")
        print("1. **é¢„æµ‹æ€§æ‰¹å¤„ç†**: æ ¹æ®å†å²æ•°æ®é¢„æµ‹æœ€ä¼˜æ‰¹å¤§å°")
        print("2. **æ™ºèƒ½é˜Ÿåˆ—ç®¡ç†**: åŸºäºä¼˜å…ˆçº§å’Œé¢„æµ‹çš„è°ƒåº¦")
        print("3. **åŠ¨æ€èµ„æºåˆ†é…**: æ ¹æ®è´Ÿè½½é¢„æµ‹è°ƒæ•´èµ„æº")
        print("4. **è‡ªé€‚åº”ç­–ç•¥**: å®æ—¶å­¦ä¹ æœ€ä¼˜å‚æ•°")

        print("\nğŸ¯ **é¢„æœŸæ€§èƒ½æå‡:**")
        print("â€¢ TTFTé™ä½: 20-40%")
        print("â€¢ ååé‡æå‡: 15-30%")
        print("â€¢ èµ„æºåˆ©ç”¨ç‡: 25-50%æå‡")
        print("â€¢ å»¶è¿Ÿç¨³å®šæ€§: æ˜¾è‘—æ”¹å–„")

        print("\nğŸ“ **ç”Ÿæˆçš„åˆ†ææ–‡ä»¶:**")
        print("â€¢ output/current_system_limitations.png - æ€§èƒ½å¯¹æ¯”å›¾")
        print("â€¢ output/scheduler_efficiency_analysis.png - æ•ˆç‡åˆ†æå›¾")
        print("â€¢ output/limitation_analysis_report.md - è¯¦ç»†åˆ†ææŠ¥å‘Š")

    except FileNotFoundError:
        print("âŒ æœªæ‰¾åˆ°baselineå®éªŒç»“æœæ–‡ä»¶")
        print("è¯·å…ˆè¿è¡Œbaselineå®éªŒ:")
        print("  PYTHONPATH=/mnt/f/LLMServingSim python3 docs/main.py --model_name 'meta-llama/Llama-3.1-8B-Instruct' --hardware 'RTX3090' --npu_num 1 --npu_group 1 --dataset 'dataset/BurstGPT_1.csv' --output 'output/baseline_tsv_full.csv'")

def show_scheduler_insights():
    """æ˜¾ç¤ºè°ƒåº¦å™¨å†…éƒ¨æ´å¯Ÿ"""
    print("\n" + "="*80)
    print("ğŸ§  **è°ƒåº¦å™¨å†…éƒ¨å†³ç­–åˆ†æ**")
    print("="*80)

    print("\nä»è°ƒåº¦æ—¥å¿—è§‚å¯Ÿåˆ°çš„å…³é”®æ¨¡å¼:")
    print("1. **è´ªå©ªæ‰¹å¤„ç†ç­–ç•¥**:")
    print("   â€¢ æ—©æœŸ: å°æ‰¹é‡(1-2ä¸ªè¯·æ±‚) â†’ èµ„æºæµªè´¹")
    print("   â€¢ ä¸­æœŸ: ä¸­ç­‰æ‰¹é‡(3-12ä¸ªè¯·æ±‚) â†’ é€æ­¥ä¼˜åŒ–")
    print("   â€¢ åæœŸ: å¤§æ‰¹é‡(æœ€å¤š88ä¸ªè¯·æ±‚) â†’ å¯èƒ½è¿‡è½½")

    print("\n2. **å†…å­˜ç®¡ç†é—®é¢˜**:")
    print("   â€¢ KVç¼“å­˜æŒç»­å¢é•¿(472â†’3000+)")
    print("   â€¢ ç¼ºä¹ä¸»åŠ¨å†…å­˜å›æ”¶æœºåˆ¶")
    print("   â€¢ å†…å­˜ç¢ç‰‡åŒ–ä¸¥é‡")

    print("\n3. **ååé‡æ³¢åŠ¨**:")
    print("   â€¢ æç¤ºé˜¶æ®µ: 0-670 tokens/s (å¤§å¹…æ³¢åŠ¨)")
    print("   â€¢ ç”Ÿæˆé˜¶æ®µ: 94-190 tokens/s (ç›¸å¯¹ç¨³å®š)")
    print("   â€¢ ç¼ºä¹å¹³æ»‘æœºåˆ¶")

    print("\n4. **è°ƒåº¦å†³ç­–ä¾æ®**:")
    print("   â€¢ ä»…åŸºäºå½“å‰é˜Ÿåˆ—çŠ¶æ€")
    print("   â€¢ æ— å†å²è´Ÿè½½è€ƒè™‘")
    print("   â€¢ æ— æœªæ¥è´Ÿè½½é¢„æµ‹")

def main():
    """ä¸»å‡½æ•°"""
    print_limitation_summary()
    show_scheduler_insights()

    print("\n" + "="*80)
    print("âœ… **åˆ†æå®Œæˆ - ä¸‹ä¸€æ­¥å»ºè®®**")
    print("="*80)
    print("\nğŸ”§ **ç«‹å³å¯è¡Œçš„æ”¹è¿›:**")
    print("1. å®ç°åŸºäºæ—¶é—´åºåˆ—çš„è´Ÿè½½é¢„æµ‹")
    print("2. å¼€å‘åŠ¨æ€æ‰¹å¤„ç†ç®—æ³•")
    print("3. ä¼˜åŒ–å†…å­˜ç®¡ç†å’Œå›æ”¶ç­–ç•¥")
    print("4. å®ç°ä¼˜å…ˆçº§è°ƒåº¦æœºåˆ¶")

    print("\nğŸ“ **è®ºæ–‡è´¡çŒ®ç‚¹:**")
    print("1. é¦–æ¬¡å°†é¢„æµ‹è°ƒåº¦åº”ç”¨äºLLMæœåŠ¡")
    print("2. çªå‘å·¥ä½œè´Ÿè½½å»ºæ¨¡å’Œåˆ†æ")
    print("3. å¤šç›®æ ‡ä¼˜åŒ–(å»¶è¿Ÿã€ååé‡ã€å†…å­˜)")
    print("4. å®æ—¶è‡ªé€‚åº”è°ƒåº¦ç³»ç»Ÿ")

if __name__ == "__main__":
    main()