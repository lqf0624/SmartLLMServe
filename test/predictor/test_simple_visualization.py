#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆå¯è§†åŒ–æ¨¡å—æµ‹è¯•è„šæœ¬

æµ‹è¯•ç®€åŒ–çš„é¢„æµ‹å¯¹æ¯”å’ŒåŸºç¡€æŒ‡æ ‡åŠŸèƒ½ã€‚
"""

import pandas as pd
import numpy as np
import logging
import sys
from pathlib import Path
import matplotlib.pyplot as plt

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent.parent))

from predictor.workload_predictor import WorkloadPredictor, ModelType, PredictionHorizon, PredictionResult
from predictor.simple_visualization import SimplePredictionVisualizer

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def create_test_data(size: int = 500) -> pd.DataFrame:
    """åˆ›å»ºæµ‹è¯•æ•°æ®"""
    logger.info(f"åˆ›å»ºæµ‹è¯•æ•°æ®ï¼Œå¤§å°: {size}")

    # ç”Ÿæˆæ—¶é—´æˆ³
    base_time = pd.Timestamp('2023-01-01')
    timestamps = [base_time + pd.Timedelta(milliseconds=i*100) for i in range(size)]

    # ç”Ÿæˆæœ‰è¶‹åŠ¿å’Œå­£èŠ‚æ€§çš„æ•°æ®
    t = np.arange(size)
    trend = 30 + 0.05 * t
    seasonal = 8 * np.sin(2 * np.pi * t / 80) + 4 * np.sin(2 * np.pi * t / 25)
    noise = np.random.normal(0, 3, size)

    input_toks = np.maximum(10, trend + seasonal + noise).astype(int)
    output_toks = (input_toks * np.random.uniform(2, 4, size)).astype(int)

    return pd.DataFrame({
        'arrival_time_ns': timestamps,
        'input_toks': input_toks,
        'output_toks': output_toks,
        'model_type': ['ChatGPT'] * size
    })


def create_prediction_result(test_data: pd.DataFrame, num_predictions: int = 20) -> PredictionResult:
    """åˆ›å»ºæ¨¡æ‹Ÿé¢„æµ‹ç»“æœ"""
    logger.info(f"åˆ›å»ºé¢„æµ‹ç»“æœï¼Œé¢„æµ‹æ•°é‡: {num_predictions}")

    # è·å–æœ€åæ—¶é—´æˆ³
    last_time = pd.to_datetime(test_data['arrival_time_ns'].iloc[-1])

    # ç”Ÿæˆé¢„æµ‹è¯·æ±‚
    predicted_requests = []
    for i in range(num_predictions):
        pred_time = last_time + pd.Timedelta(seconds=i*5)
        predicted_requests.append({
            'request_id': f'pred_{i}',
            'arrival_time_ns': pred_time.value,
            'input_tokens': np.random.randint(40, 120),
            'output_tokens': np.random.randint(80, 250),
            'burst_pattern': 'steady',
            'model_type': 'ChatGPT',
            'request_type': 'short',
            'priority': 'medium',
            'estimated_compute_time': 0.08,
            'memory_requirement_mb': 80.0,
            'predicted_accuracy': 0.82
        })

    return PredictionResult(
        predicted_requests=predicted_requests,
        confidence=0.82,
        prediction_metadata={
            'model_type': 'Test Model',
            'prediction_horizon': 'short_term',
            'sequence_length': 50
        }
    )


def test_simple_visualization():
    """æµ‹è¯•ç®€åŒ–ç‰ˆå¯è§†åŒ–åŠŸèƒ½"""
    logger.info("=== æµ‹è¯•ç®€åŒ–ç‰ˆå¯è§†åŒ–åŠŸèƒ½ ===")

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_data = create_test_data(400)

    # åˆ›å»ºå¯è§†åŒ–å™¨
    visualizer = SimplePredictionVisualizer()

    # åˆ›å»ºé¢„æµ‹ç»“æœ
    prediction_result = create_prediction_result(test_data, 15)

    try:
        # æµ‹è¯•åŸºç¡€é¢„æµ‹å¯¹æ¯”
        logger.info("æµ‹è¯•åŸºç¡€é¢„æµ‹å¯¹æ¯”...")
        metrics = visualizer.plot_prediction_comparison(
            test_data, prediction_result, "Test Model",
            save_path="test_simple_prediction.png",
            show_plot=False
        )

        logger.info(f"åŸºç¡€æŒ‡æ ‡: {metrics}")

        # æµ‹è¯•å¤šæ¨¡å‹å¯¹æ¯”
        logger.info("æµ‹è¯•å¤šæ¨¡å‹å¯¹æ¯”...")
        prediction_results = {
            'model1': create_prediction_result(test_data, 15),
            'model2': create_prediction_result(test_data, 15)
        }

        model_names = {'model1': 'LSTM', 'model2': 'DLinear'}
        multi_metrics = visualizer.plot_multi_model_comparison(
            prediction_results, test_data, model_names,
            save_path="test_multi_model_comparison.png",
            show_plot=False
        )

        logger.info(f"å¤šæ¨¡å‹æŒ‡æ ‡: {multi_metrics}")

        # æµ‹è¯•æŠ¥å‘Šç”Ÿæˆ
        logger.info("æµ‹è¯•æŠ¥å‘Šç”Ÿæˆ...")
        report = visualizer.generate_simple_report(
            prediction_results, test_data, model_names,
            save_path="test_prediction_report.txt"
        )

        logger.info(f"æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼Œé•¿åº¦: {len(report)} å­—ç¬¦")

        logger.info("âœ… ç®€åŒ–ç‰ˆå¯è§†åŒ–åŠŸèƒ½æµ‹è¯•é€šè¿‡")

    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

    return True


def test_lstm_prediction():
    """æµ‹è¯•LSTMé¢„æµ‹å™¨çš„ç®€åŒ–å¯è§†åŒ–"""
    logger.info("=== æµ‹è¯•LSTMé¢„æµ‹å™¨ç®€åŒ–å¯è§†åŒ– ===")

    try:
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_data = create_test_data(300)

        # åˆ›å»ºLSTMé¢„æµ‹å™¨
        predictor = WorkloadPredictor(
            model_type=ModelType.LSTM,
            prediction_horizon=PredictionHorizon.SHORT_TERM,
            sequence_length=50
        )

        # å¿«é€Ÿè®­ç»ƒ
        logger.info("è®­ç»ƒLSTMæ¨¡å‹...")
        training_results = predictor.train(test_data, validation_split=0.2)
        logger.info(f"è®­ç»ƒå®Œæˆï¼Œæœ€ç»ˆæŸå¤±: {training_results.get('final_val_loss', 'N/A')}")

        # è¿›è¡Œé¢„æµ‹
        historical_data = test_data.iloc[:-20]
        prediction_result = predictor.predict(historical_data, steps=10)

        # ä½¿ç”¨ç®€åŒ–å¯è§†åŒ–
        visualizer = SimplePredictionVisualizer()
        metrics = visualizer.plot_prediction_comparison(
            test_data, prediction_result, "LSTM Model",
            save_path="test_lstm_prediction.png",
            show_plot=False
        )

        logger.info(f"LSTMé¢„æµ‹æŒ‡æ ‡: {metrics}")

        logger.info("âœ… LSTMé¢„æµ‹å™¨ç®€åŒ–å¯è§†åŒ–æµ‹è¯•é€šè¿‡")

    except Exception as e:
        logger.error(f"âŒ LSTMé¢„æµ‹å™¨æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

    return True


def cleanup_test_files():
    """æ¸…ç†æµ‹è¯•æ–‡ä»¶"""
    import os
    test_files = [
        "test_simple_prediction.png",
        "test_multi_model_comparison.png",
        "test_lstm_prediction.png",
        "test_prediction_report.txt"
    ]
    for file in test_files:
        if os.path.exists(file):
            os.remove(file)
            logger.info(f"æ¸…ç†æµ‹è¯•æ–‡ä»¶: {file}")


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    logger.info("å¼€å§‹ç®€åŒ–ç‰ˆå¯è§†åŒ–æµ‹è¯•...")

    try:
        # è¿è¡Œæµ‹è¯•
        success1 = test_simple_visualization()
        success2 = test_lstm_prediction()

        if success1 and success2:
            logger.info("ğŸ‰ æ‰€æœ‰ç®€åŒ–ç‰ˆå¯è§†åŒ–æµ‹è¯•é€šè¿‡ï¼")
            return True
        else:
            logger.error("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥")
            return False

    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•è¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return False

    finally:
        # æ¸…ç†æµ‹è¯•æ–‡ä»¶
        cleanup_test_files()


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)