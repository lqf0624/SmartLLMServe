#!/usr/bin/env python3
"""
å¤šæ¨¡å‹é›†æˆé¢„æµ‹å™¨æµ‹è¯•è„šæœ¬

æµ‹è¯•æ–°çš„EnsemblePredictorç±»å’ŒWorkloadPredictorçš„é›†æˆåŠŸèƒ½ã€‚
"""

import pandas as pd
import numpy as np
import logging
import sys
from pathlib import Path
import matplotlib.pyplot as plt

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent.parent))

from predictor.workload_predictor import WorkloadPredictor, ModelType, PredictionHorizon, PredictionResult
from predictor.ensemble_predictor import EnsemblePredictor, WeightStrategy
from predictor.simple_visualization import SimplePredictionVisualizer

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def create_test_data(size: int = 500) -> pd.DataFrame:
    """åˆ›å»ºæµ‹è¯•æ•°æ®"""
    logger.info(f"åˆ›å»ºæµ‹è¯•æ•°æ®ï¼Œå¤§å°: {size}")

    # ç”Ÿæˆæ—¶é—´æˆ³
    base_time = pd.Timestamp('2023-01-01')
    timestamps = [base_time + pd.Timedelta(milliseconds=i*100) for i in range(size)]

    # ç”Ÿæˆæœ‰è¶‹åŠ¿å’Œå­£èŠ‚æ€§çš„æ•°æ®
    t = np.arange(size)
    trend = 30 + 0.1 * t
    seasonal = 8 * np.sin(2 * np.pi * t / 80) + 4 * np.sin(2 * np.pi * t / 25)
    noise = np.random.normal(0, 3, size)

    input_toks = np.maximum(15, trend + seasonal + noise).astype(int)
    output_toks = (input_toks * np.random.uniform(2, 4, size)).astype(int)

    return pd.DataFrame({
        'arrival_time_ns': timestamps,
        'input_toks': input_toks,
        'output_toks': output_toks,
        'model_type': ['ChatGPT'] * size
    })


def test_ensemble_predictor_standalone():
    """æµ‹è¯•ç‹¬ç«‹çš„EnsemblePredictor"""
    logger.info("=== æµ‹è¯•ç‹¬ç«‹EnsemblePredictor ===")

    try:
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_data = create_test_data(400)

        # æµ‹è¯•ä¸åŒçš„æƒé‡ç­–ç•¥
        strategies = [
            WeightStrategy.STATIC,
            WeightStrategy.PERFORMANCE_BASED,
            WeightStrategy.DYNAMIC,
            WeightStrategy.ADAPTIVE
        ]

        results = {}

        for strategy in strategies:
            logger.info(f"æµ‹è¯•æƒé‡ç­–ç•¥: {strategy.value}")

            # åˆ›å»ºé›†æˆé¢„æµ‹å™¨
            ensemble_predictor = EnsemblePredictor(
                models=[ModelType.LSTM, ModelType.DLINEAR],
                weight_strategy=strategy,
                prediction_horizon=PredictionHorizon.SHORT_TERM,
                sequence_length=40,
                performance_window=50
            )

            # è®­ç»ƒæ¨¡å‹
            training_results = ensemble_predictor.train(test_data, validation_split=0.2)
            logger.info(f"è®­ç»ƒå®Œæˆï¼Œæƒé‡: {training_results.get('ensemble_weights', {})}")

            # è¿›è¡Œé¢„æµ‹
            historical_data = test_data.iloc[:-30]
            prediction_result = ensemble_predictor.predict(historical_data, steps=10)

            logger.info(f"é¢„æµ‹å®Œæˆï¼Œè¯·æ±‚æ•°é‡: {len(prediction_result.predicted_requests)}")
            logger.info(f"é¢„æµ‹ç½®ä¿¡åº¦: {prediction_result.confidence:.3f}")

            results[strategy.value] = {
                'training_results': training_results,
                'prediction_result': prediction_result,
                'ensemble_info': ensemble_predictor.get_ensemble_info()
            }

        # å¯¹æ¯”ä¸åŒç­–ç•¥çš„ç»“æœ
        logger.info("\nç­–ç•¥å¯¹æ¯”åˆ†æ:")
        for strategy_name, result in results.items():
            weights = result['ensemble_info']['weights']
            confidence = result['prediction_result'].confidence
            logger.info(f"  {strategy_name}: æƒé‡={weights}, ç½®ä¿¡åº¦={confidence:.3f}")

        logger.info("âœ… ç‹¬ç«‹EnsemblePredictoræµ‹è¯•é€šè¿‡")
        return True

    except Exception as e:
        logger.error(f"âŒ ç‹¬ç«‹EnsemblePredictoræµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_workload_predictor_ensemble():
    """æµ‹è¯•WorkloadPredictorçš„é›†æˆåŠŸèƒ½"""
    logger.info("=== æµ‹è¯•WorkloadPredictoré›†æˆåŠŸèƒ½ ===")

    try:
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_data = create_test_data(300)

        # åˆ›å»ºå¯ç”¨é›†æˆçš„WorkloadPredictor
        predictor = WorkloadPredictor(
            model_type=ModelType.ENSEMBLE,
            prediction_horizon=PredictionHorizon.SHORT_TERM,
            sequence_length=40,
            enable_ensemble=True,
            ensemble_strategy=WeightStrategy.PERFORMANCE_BASED
        )

        logger.info(f"WorkloadPredictoråˆå§‹åŒ–å®Œæˆï¼Œæ¿€æ´»æ¨¡å‹: {[m.value for m in predictor.active_models]}")

        # è®­ç»ƒæ¨¡å‹
        logger.info("å¼€å§‹è®­ç»ƒ...")
        training_results = predictor.train(test_data, validation_split=0.2)

        logger.info("è®­ç»ƒç»“æœ:")
        for model_name, result in training_results.items():
            if isinstance(result, dict):
                status = result.get('status', 'unknown')
                logger.info(f"  {model_name}: {status}")
                if 'ensemble_weights' in result:
                    logger.info(f"    æƒé‡: {result['ensemble_weights']}")

        # è¿›è¡Œé¢„æµ‹
        logger.info("\nå¼€å§‹é¢„æµ‹...")
        historical_data = test_data.iloc[:-20]
        prediction_result = predictor.predict(historical_data, steps=8)

        logger.info(f"é¢„æµ‹å®Œæˆ:")
        logger.info(f"  è¯·æ±‚æ•°é‡: {len(prediction_result.predicted_requests)}")
        logger.info(f"  ç½®ä¿¡åº¦: {prediction_result.confidence:.3f}")
        logger.info(f"  é¢„æµ‹å…ƒæ•°æ®: {list(prediction_result.prediction_metadata.keys())}")

        # æµ‹è¯•å¯è§†åŒ–
        logger.info("\nç”Ÿæˆå¯è§†åŒ–...")
        visualizer = SimplePredictionVisualizer()
        metrics = visualizer.plot_prediction_comparison(
            test_data, prediction_result, "Ensemble Model",
            save_path="test_ensemble_prediction.png",
            show_plot=False
        )

        logger.info(f"åŸºç¡€æŒ‡æ ‡: {metrics}")

        logger.info("âœ… WorkloadPredictoré›†æˆåŠŸèƒ½æµ‹è¯•é€šè¿‡")
        return True

    except Exception as e:
        logger.error(f"âŒ WorkloadPredictoré›†æˆåŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_multi_strategy_comparison():
    """æµ‹è¯•å¤šç­–ç•¥å¯¹æ¯”"""
    logger.info("=== æµ‹è¯•å¤šç­–ç•¥å¯¹æ¯” ===")

    try:
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_data = create_test_data(350)

        strategies = [
            WeightStrategy.STATIC,
            WeightStrategy.PERFORMANCE_BASED,
            WeightStrategy.ADAPTIVE
        ]

        strategy_results = {}

        for strategy in strategies:
            logger.info(f"\næµ‹è¯•ç­–ç•¥: {strategy.value}")

            # åˆ›å»ºWorkloadPredictor
            predictor = WorkloadPredictor(
                model_type=ModelType.ENSEMBLE,
                prediction_horizon=PredictionHorizon.SHORT_TERM,
                sequence_length=35,
                enable_ensemble=True,
                ensemble_strategy=strategy
            )

            # è®­ç»ƒ
            training_results = predictor.train(test_data, validation_split=0.25)

            # é¢„æµ‹
            historical_data = test_data.iloc[:-25]
            prediction_result = predictor.predict(historical_data, steps=12)

            # æ”¶é›†ç»“æœ
            strategy_results[strategy.value] = {
                'training_results': training_results,
                'prediction_result': prediction_result,
                'active_models': predictor.active_models,
                'weights': predictor.ensemble_weights
            }

        # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
        logger.info("\n=== å¤šç­–ç•¥å¯¹æ¯”æŠ¥å‘Š ===")
        for strategy_name, result in strategy_results.items():
            weights = result['weights']
            confidence = result['prediction_result'].confidence
            request_count = len(result['prediction_result'].predicted_requests)

            logger.info(f"\n{strategy_name}:")
            logger.info(f"  æ´»è·ƒæ¨¡å‹: {[m.value for m in result['active_models']]}")
            logger.info(f"  æƒé‡åˆ†é…: {weights}")
            logger.info(f"  é¢„æµ‹ç½®ä¿¡åº¦: {confidence:.3f}")
            logger.info(f"  é¢„æµ‹è¯·æ±‚æ•°: {request_count}")

        # æ‰¾å‡ºæœ€ä½³ç­–ç•¥
        best_strategy = max(strategy_results.keys(),
                          key=lambda x: strategy_results[x]['prediction_result'].confidence)
        logger.info(f"\nğŸ† æœ€ä½³ç­–ç•¥: {best_strategy} (ç½®ä¿¡åº¦: {strategy_results[best_strategy]['prediction_result'].confidence:.3f})")

        logger.info("âœ… å¤šç­–ç•¥å¯¹æ¯”æµ‹è¯•é€šè¿‡")
        return True

    except Exception as e:
        logger.error(f"âŒ å¤šç­–ç•¥å¯¹æ¯”æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def cleanup_test_files():
    """æ¸…ç†æµ‹è¯•æ–‡ä»¶"""
    import os
    test_files = ["test_ensemble_prediction.png"]
    for file in test_files:
        if os.path.exists(file):
            os.remove(file)
            logger.info(f"æ¸…ç†æµ‹è¯•æ–‡ä»¶: {file}")


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    logger.info("å¼€å§‹å¤šæ¨¡å‹é›†æˆé¢„æµ‹å™¨æµ‹è¯•...")

    try:
        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
        test1 = test_ensemble_predictor_standalone()
        test2 = test_workload_predictor_ensemble()
        test3 = test_multi_strategy_comparison()

        if test1 and test2 and test3:
            logger.info("ğŸ‰ æ‰€æœ‰å¤šæ¨¡å‹é›†æˆé¢„æµ‹å™¨æµ‹è¯•é€šè¿‡ï¼")
            return True
        else:
            logger.error("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥")
            return False

    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•è¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return False

    finally:
        # æ¸…ç†æµ‹è¯•æ–‡ä»¶
        cleanup_test_files()


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)