#!/usr/bin/env python3
"""
å¢å¼ºè¯„ä¼°æŒ‡æ ‡æµ‹è¯•è„šæœ¬

æµ‹è¯•æ–°æ·»åŠ çš„å…¨é¢è¯„ä¼°æŒ‡æ ‡åŠŸèƒ½ï¼ŒåŒ…æ‹¬é«˜çº§æŒ‡æ ‡ã€ç»Ÿè®¡æ£€éªŒã€ä¸šåŠ¡æŒ‡æ ‡å’Œé²æ£’æ€§åˆ†æã€‚
"""

import pandas as pd
import numpy as np
import logging
import sys
from pathlib import Path
import matplotlib.pyplot as plt

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent.parent))

from predictor.workload_predictor import WorkloadPredictor, ModelType, PredictionHorizon, PredictionResult
from predictor.visualization import PredictionVisualizer
from predictor.dlinear_predictor_adapter import create_dlinear_predictor

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def create_test_data(size: int = 1000) -> pd.DataFrame:
    """åˆ›å»ºæµ‹è¯•æ•°æ®"""
    logger.info(f"åˆ›å»ºæµ‹è¯•æ•°æ®ï¼Œå¤§å°: {size}")

    # ç”Ÿæˆæ—¶é—´æˆ³
    base_time = pd.Timestamp('2023-01-01')
    timestamps = [base_time + pd.Timedelta(milliseconds=i*100) for i in range(size)]

    # ç”Ÿæˆæœ‰è¶‹åŠ¿å’Œå­£èŠ‚æ€§çš„æ•°æ®
    t = np.arange(size)
    trend = 50 + 0.1 * t
    seasonal = 10 * np.sin(2 * np.pi * t / 100) + 5 * np.sin(2 * np.pi * t / 30)
    noise = np.random.normal(0, 5, size)

    input_toks = np.maximum(10, trend + seasonal + noise).astype(int)
    output_toks = (input_toks * np.random.uniform(2, 4, size)).astype(int)

    # æ·»åŠ ä¸€äº›çªå‘
    burst_positions = np.random.choice(size, size//50, replace=False)
    input_toks[burst_positions] = np.random.randint(200, 500, len(burst_positions))
    output_toks[burst_positions] = input_toks[burst_positions] * np.random.randint(2, 5, len(burst_positions))

    return pd.DataFrame({
        'arrival_time_ns': timestamps,
        'input_toks': input_toks,
        'output_toks': output_toks,
        'model_type': ['ChatGPT'] * size
    })


def test_comprehensive_metrics():
    """æµ‹è¯•å…¨é¢è¯„ä¼°æŒ‡æ ‡åŠŸèƒ½"""
    logger.info("=== æµ‹è¯•å…¨é¢è¯„ä¼°æŒ‡æ ‡åŠŸèƒ½ ===")

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_data = create_test_data(800)

    # åˆ›å»ºå¯è§†åŒ–å™¨
    visualizer = PredictionVisualizer()

    # åˆ›å»ºä¸€ä¸ªç®€å•çš„é¢„æµ‹ç»“æœï¼ˆæ¨¡æ‹Ÿï¼‰

    # æ¨¡æ‹Ÿé¢„æµ‹è¯·æ±‚
    predicted_requests = []
    base_time = pd.Timestamp('2023-01-01 00:01:00')
    for i in range(20):
        req_time = base_time + pd.Timedelta(seconds=i*5)
        predicted_requests.append({
            'request_id': f'pred_{i}',
            'arrival_time_ns': req_time.value,
            'input_tokens': np.random.randint(50, 150),
            'output_tokens': np.random.randint(100, 300),
            'burst_pattern': 'steady',
            'model_type': 'ChatGPT',
            'request_type': 'short',
            'priority': 'medium',
            'estimated_compute_time': 0.1,
            'memory_requirement_mb': 100.0,
            'predicted_accuracy': 0.85
        })

    # åˆ›å»ºé¢„æµ‹ç»“æœ
    prediction_result = PredictionResult(
        predicted_requests=predicted_requests,
        confidence=0.85,
        prediction_metadata={
            'model_type': 'LSTM',
            'prediction_horizon': 'short_term',
            'sequence_length': 50
        }
    )

    # æµ‹è¯•å…¨é¢æŒ‡æ ‡è®¡ç®—
    logger.info("æµ‹è¯•å…¨é¢æŒ‡æ ‡è®¡ç®—...")
    comprehensive_metrics = visualizer.calculate_comprehensive_metrics(
        test_data, prediction_result
    )

    logger.info(f"åŸºç¡€æŒ‡æ ‡: {comprehensive_metrics['basic_metrics']}")
    logger.info(f"é«˜çº§æŒ‡æ ‡: {comprehensive_metrics['advanced_metrics']}")
    logger.info(f"ä¸šåŠ¡æŒ‡æ ‡: {comprehensive_metrics['business_metrics']}")
    logger.info(f"é²æ£’æ€§æŒ‡æ ‡: {comprehensive_metrics['robustness_metrics']}")
    logger.info(f"æ€»ä½“è¯„åˆ†: {comprehensive_metrics['overall_score']:.3f}")

    # æµ‹è¯•æŒ‡æ ‡åˆ†æå›¾
    logger.info("æµ‹è¯•æŒ‡æ ‡åˆ†æå›¾...")
    visualizer.plot_comprehensive_metrics_analysis(
        test_data, prediction_result, "Test Model",
        save_path="test_comprehensive_metrics.png",
        show_plot=False
    )

    # æµ‹è¯•æŠ¥å‘Šç”Ÿæˆ
    logger.info("æµ‹è¯•æŠ¥å‘Šç”Ÿæˆ...")
    report = visualizer.generate_metrics_report(test_data, prediction_result, "Test Model")

    logger.info(f"æ€§èƒ½ç­‰çº§: {report['summary']['performance_grade']}")
    logger.info(f"ç½®ä¿¡æ°´å¹³: {report['summary']['confidence_level']:.2f}")
    logger.info("æ”¹è¿›å»ºè®®:")
    for rec in report['recommendations']:
        logger.info(f"  - {rec}")

    logger.info("âœ… å…¨é¢è¯„ä¼°æŒ‡æ ‡åŠŸèƒ½æµ‹è¯•é€šè¿‡")


def test_lstm_dlinear_comparison():
    """æµ‹è¯•LSTM vs DLinearçš„å…¨é¢å¯¹æ¯”"""
    logger.info("=== æµ‹è¯•LSTM vs DLinearå…¨é¢å¯¹æ¯” ===")

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_data = create_test_data(600)

    models = {
        'LSTM': ModelType.LSTM,
        'DLinear': ModelType.DLINEAR
    }

    results = {}

    for model_name, model_type in models.items():
        logger.info(f"æµ‹è¯• {model_name} æ¨¡å‹...")

        try:
            # åˆ›å»ºé¢„æµ‹å™¨
            predictor = WorkloadPredictor(
                model_type=model_type,
                prediction_horizon=PredictionHorizon.SHORT_TERM,
                sequence_length=50
            )

            # å¿«é€Ÿè®­ç»ƒ
            training_results = predictor.train(test_data, validation_split=0.2)
            logger.info(f"{model_name} è®­ç»ƒå®Œæˆ")

            # é¢„æµ‹
            historical_data = test_data.iloc[:-30]
            prediction_result = predictor.predict(historical_data, steps=10)

            # è®¡ç®—å…¨é¢æŒ‡æ ‡
            visualizer = PredictionVisualizer()
            comprehensive_metrics = visualizer.calculate_comprehensive_metrics(
                test_data, prediction_result
            )

            results[model_name] = {
                'metrics': comprehensive_metrics,
                'prediction_result': prediction_result,
                'training_results': training_results
            }

            logger.info(f"{model_name} æ€»ä½“è¯„åˆ†: {comprehensive_metrics['overall_score']:.3f}")

        except Exception as e:
            logger.error(f"{model_name} æµ‹è¯•å¤±è´¥: {e}")
            continue

    # å¯¹æ¯”åˆ†æ
    if len(results) >= 2:
        logger.info("\næ¨¡å‹å¯¹æ¯”åˆ†æ:")
        for model_name, result in results.items():
            metrics = result['metrics']
            logger.info(f"\n{model_name}:")
            logger.info(f"  æ€»ä½“è¯„åˆ†: {metrics['overall_score']:.3f}")
            logger.info(f"  åŸºç¡€æŒ‡æ ‡ MAE: {metrics['basic_metrics'].get('MAE', 'N/A'):.2f}")
            logger.info(f"  é«˜çº§æŒ‡æ ‡ SMAPE: {metrics['advanced_metrics'].get('SMAPE', 'N/A'):.1f}%")
            logger.info(f"  ä¸šåŠ¡æŒ‡æ ‡ èµ„æºæ•ˆç‡: {metrics['business_metrics'].get('Resource_Efficiency', 'N/A'):.2f}")
            logger.info(f"  é²æ£’æ€§è¯„åˆ†: {metrics['robustness_metrics'].get('Overall_Robustness', 'N/A'):.2f}")

        # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
        best_model = max(results.keys(), key=lambda x: results[x]['metrics']['overall_score'])
        logger.info(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model} (è¯„åˆ†: {results[best_model]['metrics']['overall_score']:.3f})")

    logger.info("âœ… LSTM vs DLinear å¯¹æ¯”æµ‹è¯•å®Œæˆ")


def test_metrics_visualization():
    """æµ‹è¯•æŒ‡æ ‡å¯è§†åŒ–åŠŸèƒ½"""
    logger.info("=== æµ‹è¯•æŒ‡æ ‡å¯è§†åŒ–åŠŸèƒ½ ===")

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_data = create_test_data(500)

    # åˆ›å»ºå¯è§†åŒ–å™¨
    visualizer = PredictionVisualizer()

    # åˆ›å»ºé¢„æµ‹ç»“æœ
    predicted_requests = []
    base_time = pd.Timestamp('2023-01-01 00:01:00')
    for i in range(15):
        req_time = base_time + pd.Timedelta(seconds=i*3)
        predicted_requests.append({
            'request_id': f'pred_{i}',
            'arrival_time_ns': req_time.value,
            'input_tokens': np.random.randint(40, 120),
            'output_tokens': np.random.randint(80, 250),
            'burst_pattern': 'steady',
            'model_type': 'ChatGPT',
            'request_type': 'short',
            'priority': 'medium',
            'estimated_compute_time': 0.08,
            'memory_requirement_mb': 80.0,
            'predicted_accuracy': 0.82
        })

    prediction_result = PredictionResult(
        predicted_requests=predicted_requests,
        confidence=0.82,
        prediction_metadata={
            'model_type': 'Test Model',
            'prediction_horizon': 'short_term',
            'sequence_length': 50
        }
    )

    # æµ‹è¯•å„ç§å¯è§†åŒ–
    logger.info("æµ‹è¯•å…¨é¢æŒ‡æ ‡åˆ†æå›¾...")
    visualizer.plot_comprehensive_metrics_analysis(
        test_data, prediction_result, "Visualization Test",
        save_path="test_metrics_visualization.png",
        show_plot=False
    )

    # æµ‹è¯•å¢å¼ºçš„çœŸå®vsé¢„æµ‹å¯¹æ¯”
    logger.info("æµ‹è¯•å¢å¼ºçš„çœŸå®vsé¢„æµ‹å¯¹æ¯”...")
    visualizer.plot_enhanced_actual_vs_predicted(
        test_data, prediction_result, "Enhanced Comparison",
        save_path="test_enhanced_comparison.png",
        show_plot=False
    )

    logger.info("âœ… æŒ‡æ ‡å¯è§†åŒ–åŠŸèƒ½æµ‹è¯•é€šè¿‡")


def cleanup_test_files():
    """æ¸…ç†æµ‹è¯•æ–‡ä»¶"""
    import os
    test_files = [
        "test_comprehensive_metrics.png",
        "test_metrics_visualization.png",
        "test_enhanced_comparison.png"
    ]
    for file in test_files:
        if os.path.exists(file):
            os.remove(file)
            logger.info(f"æ¸…ç†æµ‹è¯•æ–‡ä»¶: {file}")


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    logger.info("å¼€å§‹å¢å¼ºè¯„ä¼°æŒ‡æ ‡æµ‹è¯•...")

    try:
        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
        test_comprehensive_metrics()
        test_lstm_dlinear_comparison()
        test_metrics_visualization()

        logger.info("ğŸ‰ æ‰€æœ‰å¢å¼ºè¯„ä¼°æŒ‡æ ‡æµ‹è¯•é€šè¿‡ï¼")

    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

    finally:
        # æ¸…ç†æµ‹è¯•æ–‡ä»¶
        cleanup_test_files()

    return True


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)