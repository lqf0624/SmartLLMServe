#!/usr/bin/env python3
"""
æ•°æ®è´¨é‡åœºæ™¯æµ‹è¯•è„šæœ¬

æµ‹è¯•é¢„æµ‹å™¨åœ¨å„ç§æ•°æ®è´¨é‡åœºæ™¯ä¸‹çš„è¡¨ç°ï¼ŒåŒ…æ‹¬ç¼ºå¤±å€¼ã€å¼‚å¸¸å€¼ã€æ•°æ®åˆ†å¸ƒç­‰ã€‚
"""

import pandas as pd
import numpy as np
import logging
import sys
import os
import traceback
from pathlib import Path
from typing import Dict, List, Any, Tuple
import matplotlib.pyplot as plt

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent.parent))

from predictor.workload_predictor import WorkloadPredictor, ModelType, PredictionHorizon
from predictor.ensemble_predictor import EnsemblePredictor, WeightStrategy
from predictor.data_preprocessor import DataPreprocessor, DataFormat

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class DataQualityTestRunner:
    """æ•°æ®è´¨é‡æµ‹è¯•è¿è¡Œå™¨"""

    def __init__(self):
        self.test_results = {}
        self.scenarios = self._create_test_scenarios()

    def _create_base_data(self, size: int = 500) -> pd.DataFrame:
        """åˆ›å»ºåŸºç¡€æµ‹è¯•æ•°æ®"""
        base_time = pd.Timestamp('2023-01-01')
        timestamps = [base_time + pd.Timedelta(milliseconds=i*100) for i in range(size)]

        # ç”Ÿæˆç¨³å®šçš„åŸºå‡†æ•°æ®
        t = np.arange(size)
        base_input = 50 + 10 * np.sin(2 * np.pi * t / 100) + np.random.normal(0, 3, size)
        base_output = base_input * np.random.uniform(2.5, 3.5, size)

        return pd.DataFrame({
            'arrival_time_ns': timestamps,
            'input_toks': np.maximum(5, base_input).astype(int),
            'output_toks': np.maximum(10, base_output).astype(int),
            'model_type': ['ChatGPT'] * size
        })

    def _create_test_scenarios(self) -> Dict[str, pd.DataFrame]:
        """åˆ›å»ºå„ç§æµ‹è¯•åœºæ™¯"""
        scenarios = {}
        base_size = 500

        # åœºæ™¯1: å®Œç¾æ•°æ®ï¼ˆåŸºå‡†ï¼‰
        scenarios['perfect_data'] = self._create_base_data(base_size)

        # åœºæ™¯2: éšæœºç¼ºå¤±å€¼
        data_missing = self._create_base_data(base_size)
        missing_indices = np.random.choice(base_size, base_size//10, replace=False)
        data_missing.loc[missing_indices, 'input_toks'] = np.nan
        scenarios['random_missing'] = data_missing

        # åœºæ™¯3: è¿ç»­ç¼ºå¤±å€¼
        data_consecutive_missing = self._create_base_data(base_size)
        data_consecutive_missing.loc[100:120, 'input_toks'] = np.nan
        data_consecutive_missing.loc[100:120, 'output_toks'] = np.nan
        scenarios['consecutive_missing'] = data_consecutive_missing

        # åœºæ™¯4: å¼‚å¸¸å€¼ - æå¤§å€¼
        data_outliers_high = self._create_base_data(base_size)
        outlier_indices = np.random.choice(base_size, base_size//20, replace=False)
        data_outliers_high.loc[outlier_indices, 'input_toks'] = np.random.randint(500, 1000, len(outlier_indices))
        scenarios['high_outliers'] = data_outliers_high

        # åœºæ™¯5: å¼‚å¸¸å€¼ - æå°å€¼
        data_outliers_low = self._create_base_data(base_size)
        outlier_indices = np.random.choice(base_size, base_size//20, replace=False)
        data_outliers_low.loc[outlier_indices, 'input_toks'] = np.random.randint(0, 5, len(outlier_indices))
        scenarios['low_outliers'] = data_outliers_low

        # åœºæ™¯6: æ—¶é—´æˆ³å¼‚å¸¸
        data_time_anomalies = self._create_base_data(base_size)
        data_time_anomalies.loc[200:210, 'arrival_time_ns'] = None
        data_time_anomalies.loc[300, 'arrival_time_ns'] = pd.Timestamp('2022-01-01')  # è¿‡å»çš„æ—¶é—´
        scenarios['time_anomalies'] = data_time_anomalies

        # åœºæ™¯7: æ•°æ®é‡å¾ˆå°‘
        scenarios['small_dataset'] = self._create_base_data(50)

        # åœºæ™¯8: æ•°æ®åˆ†å¸ƒåæ–œ
        data_skewed = self._create_base_data(base_size)
        # åˆ›å»ºé•¿å°¾åˆ†å¸ƒ
        skewed_values = np.random.exponential(20, base_size)
        data_skewed['input_toks'] = np.maximum(5, skewed_values).astype(int)
        data_skewed['output_toks'] = (data_skewed['input_toks'] * np.random.uniform(2.5, 3.5, base_size)).astype(int)
        scenarios['skewed_distribution'] = data_skewed

        # åœºæ™¯9: é«˜æ³¢åŠ¨æ€§
        data_volatile = self._create_base_data(base_size)
        volatility = np.random.normal(0, 30, base_size)
        data_volatile['input_toks'] = np.maximum(5, data_volatile['input_toks'] + volatility).astype(int)
        data_volatile['output_toks'] = (data_volatile['input_toks'] * np.random.uniform(2.5, 3.5, base_size)).astype(int)
        scenarios['high_volatility'] = data_volatile

        # åœºæ™¯10: å¤šç§é—®é¢˜æ··åˆ
        data_mixed = self._create_base_data(base_size)
        # æ·»åŠ ç¼ºå¤±å€¼
        mixed_missing = np.random.choice(base_size, base_size//15, replace=False)
        data_mixed.loc[mixed_missing, 'input_toks'] = np.nan
        # æ·»åŠ å¼‚å¸¸å€¼
        mixed_outliers = np.random.choice(base_size, base_size//25, replace=False)
        data_mixed.loc[mixed_outliers, 'input_toks'] = np.random.randint(400, 800, len(mixed_outliers))
        # æ·»åŠ æ—¶é—´å¼‚å¸¸
        data_mixed.loc[150:155, 'arrival_time_ns'] = None
        scenarios['mixed_issues'] = data_mixed

        return scenarios

    def test_scenario_compatibility(self, scenario_name: str, data: pd.DataFrame) -> Dict[str, Any]:
        """æµ‹è¯•å•ä¸ªåœºæ™¯çš„å…¼å®¹æ€§"""
        logger.info(f"æµ‹è¯•åœºæ™¯: {scenario_name}")

        results = {
            'scenario_name': scenario_name,
            'data_size': len(data),
            'missing_values': data.isnull().sum().sum(),
            'zero_values': len(data[(data['input_toks'] <= 0) | (data['output_toks'] <= 0)]),
            'negative_values': len(data[(data['input_toks'] < 0) | (data['output_toks'] < 0)]),
            'outliers': self._detect_outliers(data),
            'time_issues': self._detect_time_issues(data)
        }

        return results

    def _detect_outliers(self, data: pd.DataFrame) -> int:
        """æ£€æµ‹å¼‚å¸¸å€¼"""
        Q1 = data['input_toks'].quantile(0.25)
        Q3 = data['input_toks'].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        return len(data[(data['input_toks'] < lower_bound) | (data['input_toks'] > upper_bound)])

    def _detect_time_issues(self, data: pd.DataFrame) -> int:
        """æ£€æµ‹æ—¶é—´é—®é¢˜"""
        issues = 0
        if data['arrival_time_ns'].isnull().any():
            issues += data['arrival_time_ns'].isnull().sum()
        # æ£€æŸ¥æ—¶é—´æˆ³æ˜¯å¦å•è°ƒé€’å¢
        try:
            time_diffs = data['arrival_time_ns'].diff().dropna()
            if (time_diffs < pd.Timedelta(0)).any():
                issues += (time_diffs < pd.Timedelta(0)).sum()
        except:
            issues += len(data)
        return issues

    def test_model_performance_on_scenario(self, scenario_name: str, data: pd.DataFrame) -> Dict[str, Any]:
        """æµ‹è¯•æ¨¡å‹åœ¨ç‰¹å®šåœºæ™¯ä¸‹çš„è¡¨ç°"""
        logger.info(f"æµ‹è¯•æ¨¡å‹åœ¨ {scenario_name} åœºæ™¯ä¸‹çš„è¡¨ç°")

        results = {}

        # æµ‹è¯•ä¸åŒæ¨¡å‹
        models_to_test = [
            (ModelType.LSTM, "LSTM"),
            (ModelType.DLINEAR, "DLinear"),
            (ModelType.ENSEMBLE, "Ensemble")
        ]

        for model_type, model_name in models_to_test:
            try:
                # åˆ›å»ºé¢„æµ‹å™¨
                predictor = WorkloadPredictor(
                    model_type=model_type,
                    prediction_horizon=PredictionHorizon.SHORT_TERM,
                    sequence_length=min(30, len(data)//2),
                    enable_ensemble=(model_type == ModelType.ENSEMBLE),
                    ensemble_strategy=WeightStrategy.STATIC,
                    device='cpu'
                )

                # è®­ç»ƒ
                training_result = predictor.train(data, validation_split=0.2)

                # é¢„æµ‹
                if len(data) > 20:
                    historical_data = data.iloc[:-10]
                    prediction_result = predictor.predict(historical_data, steps=5)
                else:
                    prediction_result = predictor.predict(data, steps=3)

                results[model_name] = {
                    'training_success': training_result,
                    'prediction_success': True,
                    'prediction_count': len(prediction_result.predicted_requests),
                    'confidence': prediction_result.confidence,
                    'error': None
                }

                logger.info(f"  {model_name}: ç½®ä¿¡åº¦ {prediction_result.confidence:.3f}")

            except Exception as e:
                results[model_name] = {
                    'training_success': False,
                    'prediction_success': False,
                    'prediction_count': 0,
                    'confidence': 0.0,
                    'error': str(e)
                }
                logger.warning(f"  {model_name}: å¤±è´¥ - {e}")

        return results

    def test_data_preprocessing_effectiveness(self, scenario_name: str, data: pd.DataFrame) -> Dict[str, Any]:
        """æµ‹è¯•æ•°æ®é¢„å¤„ç†çš„æœ‰æ•ˆæ€§"""
        logger.info(f"æµ‹è¯• {scenario_name} çš„æ•°æ®é¢„å¤„ç†æ•ˆæœ")

        try:
            # ä¿å­˜åŸå§‹æ•°æ®
            original_stats = {
                'size': len(data),
                'missing': data.isnull().sum().sum(),
                'min_input': data['input_toks'].min(),
                'max_input': data['input_toks'].max(),
                'mean_input': data['input_toks'].mean(),
                'std_input': data['input_toks'].std()
            }

            # æ•°æ®é¢„å¤„ç†
            preprocessor = DataPreprocessor()

            # ä¿å­˜ä¸´æ—¶æ–‡ä»¶
            temp_file = f"temp_{scenario_name}.csv"
            data.to_csv(temp_file, index=False)

            try:
                processed_data = preprocessor.load_and_preprocess(
                    temp_file,
                    DataFormat.CSV,
                    target_column='input_toks'
                )

                processed_stats = {
                    'size': len(processed_data),
                    'missing': processed_data.isnull().sum().sum(),
                    'min_input': processed_data['input_toks'].min(),
                    'max_input': processed_data['input_toks'].max(),
                    'mean_input': processed_data['input_toks'].mean(),
                    'std_input': processed_data['input_toks'].std()
                }

                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                if os.path.exists(temp_file):
                    os.remove(temp_file)

                return {
                    'preprocessing_success': True,
                    'original_stats': original_stats,
                    'processed_stats': processed_stats,
                    'improvements': {
                        'missing_reduction': original_stats['missing'] - processed_stats['missing'],
                        'size_change': processed_stats['size'] - original_stats['size']
                    }
                }

            except Exception as e:
                if os.path.exists(temp_file):
                    os.remove(temp_file)
                return {
                    'preprocessing_success': False,
                    'error': str(e)
                }

        except Exception as e:
            return {
                'preprocessing_success': False,
                'error': str(e)
            }

    def generate_scenario_analysis_report(self) -> str:
        """ç”Ÿæˆåœºæ™¯åˆ†ææŠ¥å‘Š"""
        report = []
        report.append("# æ•°æ®è´¨é‡åœºæ™¯æµ‹è¯•æŠ¥å‘Š")
        report.append(f"æµ‹è¯•æ—¶é—´: {pd.Timestamp.now()}")
        report.append("")
        report.append("## æµ‹è¯•åœºæ™¯æ¦‚è§ˆ")
        report.append("")

        for scenario_name, scenario_results in self.test_results.items():
            report.append(f"### {scenario_name}")
            report.append("")

            # åœºæ™¯åŸºæœ¬ä¿¡æ¯
            if 'compatibility' in scenario_results:
                comp = scenario_results['compatibility']
                report.append("**æ•°æ®ç‰¹å¾:**")
                report.append(f"- æ•°æ®é‡: {comp['data_size']}")
                report.append(f"- ç¼ºå¤±å€¼: {comp['missing_values']}")
                report.append(f"- é›¶/è´Ÿå€¼: {comp['zero_values']}")
                report.append(f"- å¼‚å¸¸å€¼: {comp['outliers']}")
                report.append(f"- æ—¶é—´é—®é¢˜: {comp['time_issues']}")
                report.append("")

            # é¢„å¤„ç†æ•ˆæœ
            if 'preprocessing' in scenario_results:
                prep = scenario_results['preprocessing']
                if prep['preprocessing_success']:
                    orig = prep['original_stats']
                    proc = prep['processed_stats']
                    imp = prep['improvements']
                    report.append("**é¢„å¤„ç†æ•ˆæœ:**")
                    report.append(f"- ç¼ºå¤±å€¼å‡å°‘: {imp['missing_reduction']}")
                    report.append(f"- æ•°æ®é‡å˜åŒ–: {imp['size_change']}")
                    report.append(f"- å¤„ç†åæ•°æ®èŒƒå›´: [{proc['min_input']}, {proc['max_input']}]")
                    report.append("")
                else:
                    report.append("**é¢„å¤„ç†æ•ˆæœ:** å¤±è´¥")
                    report.append(f"- é”™è¯¯: {prep['error']}")
                    report.append("")

            # æ¨¡å‹è¡¨ç°
            if 'model_performance' in scenario_results:
                report.append("**æ¨¡å‹è¡¨ç°:**")
                for model_name, perf in scenario_results['model_performance'].items():
                    if perf['prediction_success']:
                        report.append(f"- {model_name}: ç½®ä¿¡åº¦ {perf['confidence']:.3f}, {perf['prediction_count']} ä¸ªé¢„æµ‹")
                    else:
                        report.append(f"- {model_name}: å¤±è´¥ ({perf['error']})")
                report.append("")

            report.append("---")
            report.append("")

        return "\n".join(report)

    def run_all_scenario_tests(self) -> bool:
        """è¿è¡Œæ‰€æœ‰åœºæ™¯æµ‹è¯•"""
        logger.info("å¼€å§‹æ•°æ®è´¨é‡åœºæ™¯æµ‹è¯•...")

        passed_scenarios = 0
        total_scenarios = len(self.scenarios)

        try:
            for scenario_name, scenario_data in self.scenarios.items():
                logger.info(f"\n{'='*60}")
                logger.info(f"æµ‹è¯•åœºæ™¯: {scenario_name}")
                logger.info(f"{'='*60}")

                scenario_results = {}

                # 1. åœºæ™¯å…¼å®¹æ€§æµ‹è¯•
                compatibility_results = self.test_scenario_compatibility(scenario_name, scenario_data)
                scenario_results['compatibility'] = compatibility_results

                # 2. æ•°æ®é¢„å¤„ç†æµ‹è¯•
                preprocessing_results = self.test_data_preprocessing_effectiveness(scenario_name, scenario_data)
                scenario_results['preprocessing'] = preprocessing_results

                # 3. æ¨¡å‹è¡¨ç°æµ‹è¯•
                model_results = self.test_model_performance_on_scenario(scenario_name, scenario_data)
                scenario_results['model_performance'] = model_results

                self.test_results[scenario_name] = scenario_results

                # è¯„ä¼°åœºæ™¯æ˜¯å¦é€šè¿‡
                successful_models = [m for m in model_results.values() if m['prediction_success']]
                if len(successful_models) > 0:
                    logger.info(f"âœ… {scenario_name} åœºæ™¯æµ‹è¯•é€šè¿‡")
                    passed_scenarios += 1
                else:
                    logger.warning(f"âŒ {scenario_name} åœºæ™¯æµ‹è¯•å¤±è´¥")

            logger.info(f"\n{'='*60}")
            logger.info("æ•°æ®è´¨é‡åœºæ™¯æµ‹è¯•æ€»ç»“")
            logger.info(f"{'='*60}")
            logger.info(f"é€šè¿‡åœºæ™¯: {passed_scenarios}/{total_scenarios}")
            logger.info(f"é€šè¿‡ç‡: {passed_scenarios/total_scenarios*100:.1f}%")

            # ç”ŸæˆæŠ¥å‘Š
            report = self.generate_scenario_analysis_report()
            report_path = "data_quality_scenario_report.md"
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(report)
            logger.info(f"åœºæ™¯åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")

            if passed_scenarios >= total_scenarios * 0.8:  # 80%é€šè¿‡ç‡
                logger.info("ğŸ‰ æ•°æ®è´¨é‡åœºæ™¯æµ‹è¯•åŸºæœ¬é€šè¿‡ï¼")
                return True
            else:
                logger.error("âŒ æ•°æ®è´¨é‡åœºæ™¯æµ‹è¯•é€šè¿‡ç‡è¿‡ä½")
                return False

        except Exception as e:
            logger.error(f"âŒ åœºæ™¯æµ‹è¯•è¿‡ç¨‹å‡ºé”™: {e}")
            traceback.print_exc()
            return False

    def identify_data_quality_patterns(self) -> Dict[str, Any]:
        """è¯†åˆ«æ•°æ®è´¨é‡æ¨¡å¼"""
        logger.info("åˆ†ææ•°æ®è´¨é‡æ¨¡å¼...")

        patterns = {
            'failure_patterns': [],
            'success_patterns': [],
            'critical_issues': [],
            'recommendations': []
        }

        for scenario_name, results in self.test_results.items():
            # åˆ†æå¤±è´¥æ¨¡å¼
            if 'model_performance' in results:
                failed_models = [model for model, perf in results['model_performance'].items() if not perf['prediction_success']]
                if failed_models:
                    patterns['failure_patterns'].append({
                        'scenario': scenario_name,
                        'failed_models': failed_models,
                        'reason': 'Model training/prediction failed'
                    })

                # åˆ†ææˆåŠŸæ¨¡å¼
                successful_models = [model for model, perf in results['model_performance'].items() if perf['prediction_success']]
                if successful_models:
                    patterns['success_patterns'].append({
                        'scenario': scenario_name,
                        'successful_models': successful_models,
                        'avg_confidence': np.mean([results['model_performance'][model]['confidence'] for model in successful_models])
                    })

            # åˆ†æå…³é”®é—®é¢˜
            if 'compatibility' in results:
                comp = results['compatibility']
                if comp['missing_values'] > len(results['compatibility']) * 0.3:  # 30%ç¼ºå¤±å€¼
                    patterns['critical_issues'].append({
                        'scenario': scenario_name,
                        'issue': 'High missing values',
                        'severity': 'High'
                    })
                if comp['outliers'] > len(results['compatibility']) * 0.1:  # 10%å¼‚å¸¸å€¼
                    patterns['critical_issues'].append({
                        'scenario': scenario_name,
                        'issue': 'High outlier count',
                        'severity': 'Medium'
                    })

        # ç”Ÿæˆå»ºè®®
        if patterns['critical_issues']:
            patterns['recommendations'].append("å»ºè®®å®ç°æ›´å¼ºçš„å¼‚å¸¸å€¼æ£€æµ‹å’Œå¤„ç†æœºåˆ¶")
        if any('missing' in issue['issue'] for issue in patterns['critical_issues']):
            patterns['recommendations'].append("å»ºè®®æ”¹è¿›ç¼ºå¤±å€¼å¡«å……ç­–ç•¥")
        patterns['recommendations'].append("å»ºè®®åœ¨æ•°æ®é¢„å¤„ç†é˜¶æ®µå¢åŠ æ•°æ®è´¨é‡è¯„ä¼°")

        return patterns


def main():
    """ä¸»å‡½æ•°"""
    test_runner = DataQualityTestRunner()
    success = test_runner.run_all_scenario_tests()

    if success:
        # åˆ†ææ•°æ®è´¨é‡æ¨¡å¼
        patterns = test_runner.identify_data_quality_patterns()
        logger.info("æ•°æ®è´¨é‡æ¨¡å¼åˆ†æå®Œæˆ")

    return success


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)