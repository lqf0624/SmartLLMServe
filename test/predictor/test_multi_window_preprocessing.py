#!/usr/bin/env python3
"""
å¤šçª—å£é¢„å¤„ç†æ€§èƒ½æµ‹è¯•è„šæœ¬

æµ‹è¯•æ–°åˆ›å»ºçš„æ—¶åºçª—å£ç®¡ç†å™¨å’Œé¢„å¤„ç†é…ç½®ç®¡ç†å™¨çš„æ€§èƒ½å’ŒåŠŸèƒ½ã€‚
"""

import pandas as pd
import numpy as np
import logging
import sys
import os
import time
from pathlib import Path
import matplotlib.pyplot as plt
from concurrent.futures import ThreadPoolExecutor, as_completed

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent.parent))

from predictor.time_series_window_manager import TimeSeriesWindowManager, create_window_manager
from predictor.preprocessing_config_manager import PreprocessingConfigManager, create_config_manager
from predictor.workload_predictor import PredictionHorizon, ModelType
from predictor.data_preprocessor import DataPreprocessor, DataFormat

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def create_test_data(size: int = 2000, burst_patterns: bool = True) -> pd.DataFrame:
    """åˆ›å»ºæµ‹è¯•æ•°æ®"""
    logger.info(f"åˆ›å»ºæµ‹è¯•æ•°æ®ï¼Œå¤§å°: {size}")

    # ç”ŸæˆåŸºç¡€æ—¶é—´åºåˆ—
    base_times = np.arange(size) * 100_000_000  # 100msé—´éš”

    # ç”Ÿæˆè¯·æ±‚tokenæ•°é‡ï¼ˆå¸¦çªå‘æ¨¡å¼ï¼‰
    if burst_patterns:
        # åˆ›å»ºçªå‘æ¨¡å¼
        base_tokens = np.random.randint(50, 150, size)

        # æ·»åŠ çªå‘
        burst_positions = np.random.choice(size, size//10, replace=False)
        burst_tokens = np.random.randint(200, 500, len(burst_positions))
        base_tokens[burst_positions] = burst_tokens

        # æ·»åŠ ç¨€ç–æœŸ
        sparse_positions = np.random.choice(size, size//5, replace=False)
        sparse_tokens = np.random.randint(10, 50, len(sparse_positions))
        base_tokens[sparse_positions] = sparse_tokens
    else:
        base_tokens = np.random.randint(80, 120, size)

    # ç”Ÿæˆå“åº”tokenæ•°é‡ï¼ˆä¸è¾“å…¥ç›¸å…³ï¼‰
    output_tokens = base_tokens * np.random.uniform(2, 4, size)
    output_tokens = output_tokens.astype(int)

    # åˆ›å»ºæ•°æ®æ¡†
    data = pd.DataFrame({
        'arrival_time_ns': base_times,
        'input_toks': base_tokens,
        'output_toks': output_tokens,
        'model_type': np.random.choice(['ChatGPT', 'GPT-4'], size)
    })

    # æ·»åŠ ä¸€äº›å™ªå£°å’Œå¼‚å¸¸å€¼
    noise_indices = np.random.choice(size, size//20, replace=False)
    data.loc[noise_indices, 'input_toks'] = np.random.randint(500, 1000, len(noise_indices))

    logger.info(f"æµ‹è¯•æ•°æ®åˆ›å»ºå®Œæˆï¼Œå½¢çŠ¶: {data.shape}")
    return data

def test_window_manager_performance():
    """æµ‹è¯•çª—å£ç®¡ç†å™¨æ€§èƒ½"""
    logger.info("=== æµ‹è¯•çª—å£ç®¡ç†å™¨æ€§èƒ½ ===")

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_data = create_test_data(size=5000)

    # åˆ›å»ºçª—å£ç®¡ç†å™¨
    manager = create_window_manager()

    # æµ‹è¯•ä¸åŒæ—¶é—´èŒƒå›´
    horizons = [PredictionHorizon.SHORT_TERM, PredictionHorizon.MEDIUM_TERM,
                PredictionHorizon.LONG_TERM, PredictionHorizon.EXTENDED_TERM]

    performance_results = {}

    for horizon in horizons:
        logger.info(f"æµ‹è¯• {horizon.value} çª—å£æ€§èƒ½...")

        start_time = time.time()
        windows = manager.create_multi_horizon_windows(test_data, [horizon])
        creation_time = time.time() - start_time

        # æµ‹è¯•ç‰¹å¾æå–
        start_time = time.time()
        features = manager.extract_window_features(windows)
        feature_time = time.time() - start_time

        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = manager.get_window_statistics(windows)

        performance_results[horizon.value] = {
            'creation_time': creation_time,
            'feature_time': feature_time,
            'total_time': creation_time + feature_time,
            'window_count': stats[horizon.value]['window_count'],
            'avg_window_size': stats[horizon.value]['avg_window_size'],
            'memory_usage_mb': stats[horizon.value]['memory_usage_mb']
        }

        logger.info(f"  åˆ›å»ºæ—¶é—´: {creation_time:.3f}s")
        logger.info(f"  ç‰¹å¾æ—¶é—´: {feature_time:.3f}s")
        logger.info(f"  çª—å£æ•°é‡: {stats[horizon.value]['window_count']}")
        logger.info(f"  å¹³å‡çª—å£å¤§å°: {stats[horizon.value]['avg_window_size']:.1f}")

    # æµ‹è¯•å¤šæ—¶é—´èŒƒå›´å¹¶è¡Œå¤„ç†
    logger.info("\næµ‹è¯•å¤šæ—¶é—´èŒƒå›´å¹¶è¡Œå¤„ç†...")
    start_time = time.time()
    all_windows = manager.create_multi_horizon_windows(test_data, horizons)
    parallel_time = time.time() - start_time

    logger.info(f"å¤šæ—¶é—´èŒƒå›´å¹¶è¡Œå¤„ç†æ—¶é—´: {parallel_time:.3f}s")

    # è·å–æ€§èƒ½æŠ¥å‘Š
    performance_report = manager.get_performance_report()
    logger.info(f"ç¼“å­˜å‘½ä¸­ç‡: {performance_report['cache_statistics']['cache_hit_rate']:.3f}")

    return performance_results, performance_report

def test_config_manager_functionality():
    """æµ‹è¯•é…ç½®ç®¡ç†å™¨åŠŸèƒ½"""
    logger.info("=== æµ‹è¯•é…ç½®ç®¡ç†å™¨åŠŸèƒ½ ===")

    # åˆ›å»ºé…ç½®ç®¡ç†å™¨
    manager = create_config_manager()

    # æµ‹è¯•æ¨¡æ¿åº”ç”¨
    templates = ['fast', 'balanced', 'high_quality']
    template_results = {}

    for template in templates:
        logger.info(f"æµ‹è¯•æ¨¡æ¿: {template}")

        start_time = time.time()
        manager.apply_template(template)
        apply_time = time.time() - start_time

        # éªŒè¯é…ç½®
        is_valid, errors = manager.validate_config()

        # è·å–é…ç½®æ‘˜è¦
        summary = manager.get_config_summary()

        template_results[template] = {
            'apply_time': apply_time,
            'is_valid': is_valid,
            'errors': errors,
            'strategy': summary['strategy'],
            'feature_count': summary['feature_count']
        }

        logger.info(f"  åº”ç”¨æ—¶é—´: {apply_time:.3f}s")
        logger.info(f"  é…ç½®æœ‰æ•ˆ: {is_valid}")
        if errors:
            logger.warning(f"  é”™è¯¯: {errors}")

    # æµ‹è¯•æ•°æ®ä¼˜åŒ–
    logger.info("\næµ‹è¯•æ•°æ®ä¼˜åŒ–...")
    test_data_sizes = [100, 1000, 5000, 10000]
    optimization_results = []

    for size in test_data_sizes:
        start_time = time.time()
        manager.optimize_for_data(data_size=size, data_quality_score=0.7, computational_budget="medium")
        opt_time = time.time() - start_time

        summary = manager.get_config_summary()
        optimization_results.append({
            'data_size': size,
            'optimization_time': opt_time,
            'strategy': summary['strategy']
        })

        logger.info(f"  æ•°æ®å¤§å°: {size}, ä¼˜åŒ–æ—¶é—´: {opt_time:.3f}s")

    # æµ‹è¯•é…ç½®ä¿å­˜å’ŒåŠ è½½
    logger.info("\næµ‹è¯•é…ç½®ä¿å­˜å’ŒåŠ è½½...")
    config_path = "test_config_performance.json"

    start_time = time.time()
    manager.save_config(config_path)
    save_time = time.time() - start_time

    start_time = time.time()
    new_manager = create_config_manager()
    new_manager.load_config(config_path)
    load_time = time.time() - start_time

    logger.info(f"ä¿å­˜æ—¶é—´: {save_time:.3f}s")
    logger.info(f"åŠ è½½æ—¶é—´: {load_time:.3f}s")

    # æ¸…ç†æµ‹è¯•æ–‡ä»¶
    if os.path.exists(config_path):
        os.remove(config_path)

    return template_results, optimization_results, {'save_time': save_time, 'load_time': load_time}

def test_end_to_end_preprocessing():
    """æµ‹è¯•ç«¯åˆ°ç«¯é¢„å¤„ç†æ€§èƒ½"""
    logger.info("=== æµ‹è¯•ç«¯åˆ°ç«¯é¢„å¤„ç†æ€§èƒ½ ===")

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_data = create_test_data(size=3000)

    # åˆ›å»ºç®¡ç†å™¨
    window_manager = create_window_manager()
    config_manager = create_config_manager()

    # æµ‹è¯•ä¸åŒé…ç½®çš„ç«¯åˆ°ç«¯æ€§èƒ½
    configs = ['fast', 'balanced', 'high_quality']
    e2e_results = {}

    for config_name in configs:
        logger.info(f"æµ‹è¯•é…ç½®: {config_name}")

        # åº”ç”¨é…ç½®
        config_manager.apply_template(config_name)

        start_time = time.time()

        # 1. æ•°æ®é¢„å¤„ç†
        preprocessor = DataPreprocessor()
        processed_data = preprocessor.preprocess_data(test_data)

        # 2. çª—å£åˆ›å»º
        horizons = [PredictionHorizon.SHORT_TERM, PredictionHorizon.MEDIUM_TERM]
        windows = window_manager.create_multi_horizon_windows(processed_data, horizons)

        # 3. ç‰¹å¾æå–
        features = window_manager.extract_window_features(windows)

        total_time = time.time() - start_time

        # æ”¶é›†ç»Ÿè®¡ä¿¡æ¯
        stats = window_manager.get_window_statistics(windows)
        config_summary = config_manager.get_config_summary()

        e2e_results[config_name] = {
            'total_time': total_time,
            'processing_time_ratio': {
                'data_preprocessing': 0.2,  # ä¼°ç®—
                'window_creation': 0.5,     # ä¼°ç®—
                'feature_extraction': 0.3   # ä¼°ç®—
            },
            'total_windows': sum(stats[h.value]['window_count'] for h in horizons),
            'total_features': sum(len(f) for f in features.values()),
            'memory_usage_mb': sum(stats[h.value]['memory_usage_mb'] for h in horizons),
            'config_strategy': config_summary['strategy']
        }

        logger.info(f"  æ€»æ—¶é—´: {total_time:.3f}s")
        logger.info(f"  æ€»çª—å£æ•°: {e2e_results[config_name]['total_windows']}")
        logger.info(f"  æ€»ç‰¹å¾æ•°: {e2e_results[config_name]['total_features']}")

    return e2e_results

def test_scalability():
    """æµ‹è¯•å¯æ‰©å±•æ€§"""
    logger.info("=== æµ‹è¯•å¯æ‰©å±•æ€§ ===")

    data_sizes = [500, 1000, 2000, 5000]
    scalability_results = []

    for size in data_sizes:
        logger.info(f"æµ‹è¯•æ•°æ®å¤§å°: {size}")

        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_data = create_test_data(size=size)

        # åˆ›å»ºç®¡ç†å™¨
        window_manager = create_window_manager()
        config_manager = create_config_manager()

        config_manager.apply_template('balanced')

        start_time = time.time()

        # ç«¯åˆ°ç«¯å¤„ç†
        preprocessor = DataPreprocessor()
        processed_data = preprocessor.preprocess_data(test_data)

        horizons = [PredictionHorizon.SHORT_TERM, PredictionHorizon.MEDIUM_TERM]
        windows = window_manager.create_multi_horizon_windows(processed_data, horizons)

        total_time = time.time() - start_time

        stats = window_manager.get_window_statistics(windows)

        scalability_results.append({
            'data_size': size,
            'processing_time': total_time,
            'total_windows': sum(stats[h.value]['window_count'] for h in horizons),
            'throughput_windows_per_second': sum(stats[h.value]['window_count'] for h in horizons) / total_time if total_time > 0 else 0,
            'memory_usage_mb': sum(stats[h.value]['memory_usage_mb'] for h in horizons)
        })

        logger.info(f"  å¤„ç†æ—¶é—´: {total_time:.3f}s")
        logger.info(f"  çª—å£ååé‡: {scalability_results[-1]['throughput_windows_per_second']:.1f} çª—å£/ç§’")

    return scalability_results

def generate_performance_report(window_results, config_results, e2e_results, scalability_results):
    """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
    logger.info("=== ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š ===")

    report = {
        'test_timestamp': str(pd.Timestamp.now()),
        'window_manager_performance': window_results,
        'config_manager_performance': config_results,
        'end_to_end_performance': e2e_results,
        'scalability_analysis': scalability_results,
        'summary': {}
    }

    # è®¡ç®—æ±‡æ€»ç»Ÿè®¡
    report['summary']['fastest_config'] = min(e2e_results.items(), key=lambda x: x[1]['total_time'])[0]
    report['summary']['most_windows_config'] = max(e2e_results.items(), key=lambda x: x[1]['total_windows'])[0]
    report['summary']['avg_throughput'] = np.mean([r['throughput_windows_per_second'] for r in scalability_results])

    # ä¿å­˜æŠ¥å‘Š
    report_path = "multi_window_preprocessing_performance_report.json"
    import json
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)

    logger.info(f"æ€§èƒ½æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")

    # æ‰“å°å…³é”®ç»“æœ
    print("\n" + "="*60)
    print("æ€§èƒ½æµ‹è¯•ç»“æœæ±‡æ€»")
    print("="*60)

    print(f"\nçª—å£ç®¡ç†å™¨æ€§èƒ½:")
    for horizon, results in window_results.items():
        print(f"  {horizon}: {results['total_time']:.3f}s ({results['window_count']} çª—å£)")

    print(f"\né…ç½®ç®¡ç†å™¨æ€§èƒ½:")
    for template, results in config_results[0].items():
        print(f"  {template}: {results['apply_time']:.3f}s")

    print(f"\nç«¯åˆ°ç«¯é¢„å¤„ç†æ€§èƒ½:")
    for config, results in e2e_results.items():
        print(f"  {config}: {results['total_time']:.3f}s ({results['total_windows']} çª—å£)")

    print(f"\nå¯æ‰©å±•æ€§åˆ†æ:")
    for result in scalability_results:
        print(f"  æ•°æ®å¤§å° {result['data_size']}: {result['throughput_windows_per_second']:.1f} çª—å£/ç§’")

    print(f"\næ±‡æ€»:")
    print(f"  æœ€å¿«é…ç½®: {report['summary']['fastest_config']}")
    print(f"  æœ€å¤šåŠŸèƒ½é…ç½®: {report['summary']['most_windows_config']}")
    print(f"  å¹³å‡ååé‡: {report['summary']['avg_throughput']:.1f} çª—å£/ç§’")

    return report

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    logger.info("å¼€å§‹å¤šçª—å£é¢„å¤„ç†æ€§èƒ½æµ‹è¯•...")

    try:
        # æµ‹è¯•çª—å£ç®¡ç†å™¨æ€§èƒ½
        window_results, window_report = test_window_manager_performance()

        # æµ‹è¯•é…ç½®ç®¡ç†å™¨åŠŸèƒ½
        config_results = test_config_manager_functionality()

        # æµ‹è¯•ç«¯åˆ°ç«¯é¢„å¤„ç†æ€§èƒ½
        e2e_results = test_end_to_end_preprocessing()

        # æµ‹è¯•å¯æ‰©å±•æ€§
        scalability_results = test_scalability()

        # ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
        report = generate_performance_report(
            window_results, config_results, e2e_results, scalability_results
        )

        logger.info("ğŸ‰ å¤šçª—å£é¢„å¤„ç†æ€§èƒ½æµ‹è¯•å®Œæˆï¼")
        return True

    except Exception as e:
        logger.error(f"âŒ æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)