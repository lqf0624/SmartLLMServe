#!/usr/bin/env python3
"""
ç«¯åˆ°ç«¯é›†æˆæµ‹è¯•è„šæœ¬

æµ‹è¯•ä»æ•°æ®åŠ è½½åˆ°é¢„æµ‹è¾“å‡ºçš„å®Œæ•´å·¥ä½œæµç¨‹ï¼ŒéªŒè¯æ•´ä¸ªç³»ç»Ÿçš„é›†æˆæ€§å’Œç¨³å®šæ€§ã€‚
"""

import pandas as pd
import numpy as np
import logging
import sys
import os
import traceback
from pathlib import Path
import json
import tempfile
import shutil
from typing import Dict, List, Any, Optional

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent.parent))

from predictor.workload_predictor import WorkloadPredictor, ModelType, PredictionHorizon
from predictor.ensemble_predictor import EnsemblePredictor, WeightStrategy
from predictor.simple_visualization import SimplePredictionVisualizer
from predictor.data_preprocessor import DataPreprocessor, DataFormat

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class EndToEndTestRunner:
    """ç«¯åˆ°ç«¯æµ‹è¯•è¿è¡Œå™¨"""

    def __init__(self):
        self.test_results = {}
        self.temp_dir = tempfile.mkdtemp()
        logger.info(f"åˆ›å»ºä¸´æ—¶æµ‹è¯•ç›®å½•: {self.temp_dir}")

    def cleanup(self):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        if os.path.exists(self.temp_dir):
            shutil.rmtree(self.temp_dir)
            logger.info("æ¸…ç†ä¸´æ—¶æµ‹è¯•ç›®å½•")

    def create_realistic_test_data(self, size: int = 1000) -> pd.DataFrame:
        """åˆ›å»ºçœŸå®çš„æµ‹è¯•æ•°æ®"""
        logger.info(f"åˆ›å»ºçœŸå®æµ‹è¯•æ•°æ®ï¼Œå¤§å°: {size}")

        # æ¨¡æ‹ŸçœŸå®å·¥ä½œè´Ÿè½½æ•°æ®
        base_time = pd.Timestamp('2023-01-01 00:00:00')
        timestamps = []
        current_time = base_time

        # æ¨¡æ‹Ÿä¸åŒçš„å·¥ä½œè´Ÿè½½æ¨¡å¼
        input_tokens = []
        output_tokens = []
        burst_patterns = []

        for i in range(size):
            # æ—¶é—´é—´éš”ï¼ˆæ¯«ç§’ï¼‰
            if i % 100 < 10:  # çªå‘æœŸ
                interval = np.random.exponential(10)  # 10mså¹³å‡é—´éš”
                pattern = 'burst'
            elif i % 100 < 70:  # ç¨³å®šæœŸ
                interval = np.random.exponential(100)  # 100mså¹³å‡é—´éš”
                pattern = 'steady'
            else:  # ç¨€ç–æœŸ
                interval = np.random.exponential(500)  # 500mså¹³å‡é—´éš”
                pattern = 'sparse'

            current_time += pd.Timedelta(milliseconds=interval)
            timestamps.append(current_time)

            # Tokenæ•°é‡
            if pattern == 'burst':
                input_tok = np.random.randint(50, 200)
            elif pattern == 'steady':
                input_tok = np.random.randint(20, 80)
            else:
                input_tok = np.random.randint(5, 30)

            output_tok = int(input_tok * np.random.uniform(2.5, 4.0))

            input_tokens.append(input_tok)
            output_tokens.append(output_tok)
            burst_patterns.append(pattern)

        return pd.DataFrame({
            'arrival_time_ns': timestamps,
            'input_toks': input_tokens,
            'output_toks': output_tokens,
            'burst_pattern': burst_patterns,
            'model_type': ['ChatGPT'] * size
        })

    def test_data_preprocessing_workflow(self) -> bool:
        """æµ‹è¯•æ•°æ®é¢„å¤„ç†å·¥ä½œæµç¨‹"""
        logger.info("=== æµ‹è¯•æ•°æ®é¢„å¤„ç†å·¥ä½œæµç¨‹ ===")

        try:
            # åˆ›å»ºåŸå§‹æ•°æ®
            raw_data = self.create_realistic_test_data(500)

            # æ·»åŠ ä¸€äº›æ•°æ®è´¨é‡é—®é¢˜
            raw_data.loc[10, 'input_toks'] = np.nan  # ç¼ºå¤±å€¼
            raw_data.loc[20, 'output_toks'] = -1     # å¼‚å¸¸å€¼
            raw_data.loc[30, 'arrival_time_ns'] = None  # æ—¶é—´æˆ³ç¼ºå¤±

            # ä¿å­˜åŸå§‹æ•°æ®
            raw_data_path = os.path.join(self.temp_dir, "raw_data.csv")
            raw_data.to_csv(raw_data_path, index=False)

            # æ•°æ®é¢„å¤„ç†
            preprocessor = DataPreprocessor()
            processed_data = preprocessor.load_and_preprocess(
                raw_data_path,
                DataFormat.CSV,
                target_column='input_toks'
            )

            logger.info(f"åŸå§‹æ•°æ®å¤§å°: {len(raw_data)}")
            logger.info(f"å¤„ç†åæ•°æ®å¤§å°: {len(processed_data)}")

            # éªŒè¯æ•°æ®è´¨é‡
            assert not processed_data.isnull().any().any(), "å¤„ç†åä¸åº”æœ‰ç¼ºå¤±å€¼"
            assert (processed_data['input_toks'] > 0).all(), "input_toksåº”è¯¥éƒ½æ˜¯æ­£å€¼"
            assert (processed_data['output_toks'] > 0).all(), "output_toksåº”è¯¥éƒ½æ˜¯æ­£å€¼"

            # ä¿å­˜å¤„ç†åçš„æ•°æ®
            processed_data_path = os.path.join(self.temp_dir, "processed_data.csv")
            processed_data.to_csv(processed_data_path, index=False)

            self.test_results['data_preprocessing'] = {
                'success': True,
                'original_size': len(raw_data),
                'processed_size': len(processed_data),
                'missing_values_handled': raw_data.isnull().sum().sum(),
                'anomaly_values_handled': len(raw_data[(raw_data['input_toks'] <= 0) | (raw_data['output_toks'] <= 0)])
            }

            logger.info("âœ… æ•°æ®é¢„å¤„ç†å·¥ä½œæµç¨‹æµ‹è¯•é€šè¿‡")
            return True

        except Exception as e:
            logger.error(f"âŒ æ•°æ®é¢„å¤„ç†å·¥ä½œæµç¨‹æµ‹è¯•å¤±è´¥: {e}")
            traceback.print_exc()
            self.test_results['data_preprocessing'] = {'success': False, 'error': str(e)}
            return False

    def test_model_training_workflow(self) -> bool:
        """æµ‹è¯•æ¨¡å‹è®­ç»ƒå·¥ä½œæµç¨‹"""
        logger.info("=== æµ‹è¯•æ¨¡å‹è®­ç»ƒå·¥ä½œæµç¨‹ ===")

        try:
            # åˆ›å»ºæµ‹è¯•æ•°æ®
            test_data = self.create_realistic_test_data(800)

            # æµ‹è¯•ä¸åŒæ¨¡å‹çš„è®­ç»ƒ
            models_config = [
                (ModelType.LSTM, "LSTMæ¨¡å‹"),
                (ModelType.DLINEAR, "DLinearæ¨¡å‹"),
                (ModelType.ENSEMBLE, "é›†æˆæ¨¡å‹")
            ]

            training_results = {}

            for model_type, model_name in models_config:
                logger.info(f"è®­ç»ƒ {model_name}...")

                # åˆ›å»ºé¢„æµ‹å™¨
                predictor = WorkloadPredictor(
                    model_type=model_type,
                    prediction_horizon=PredictionHorizon.SHORT_TERM,
                    sequence_length=40,
                    enable_ensemble=(model_type == ModelType.ENSEMBLE),
                    ensemble_strategy=WeightStrategy.PERFORMANCE_BASED,
                    device='cpu'
                )

                # è®­ç»ƒ
                training_result = predictor.train(test_data, validation_split=0.2)

                # ä¿å­˜æ¨¡å‹
                model_path = os.path.join(self.temp_dir, f"{model_name.lower()}_model.pkl")
                predictor.save(model_path)

                training_results[model_name] = {
                    'training_success': training_result,
                    'model_saved': os.path.exists(model_path),
                    'model_info': predictor.get_model_info()
                }

                logger.info(f"{model_name} è®­ç»ƒå®Œæˆ")

            self.test_results['model_training'] = training_results

            # éªŒè¯æ‰€æœ‰æ¨¡å‹éƒ½è®­ç»ƒæˆåŠŸ
            all_success = all(result['training_success'] for result in training_results.values())
            assert all_success, "æ‰€æœ‰æ¨¡å‹éƒ½åº”è¯¥è®­ç»ƒæˆåŠŸ"

            logger.info("âœ… æ¨¡å‹è®­ç»ƒå·¥ä½œæµç¨‹æµ‹è¯•é€šè¿‡")
            return True

        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹è®­ç»ƒå·¥ä½œæµç¨‹æµ‹è¯•å¤±è´¥: {e}")
            traceback.print_exc()
            self.test_results['model_training'] = {'success': False, 'error': str(e)}
            return False

    def test_prediction_workflow(self) -> bool:
        """æµ‹è¯•é¢„æµ‹å·¥ä½œæµç¨‹"""
        logger.info("=== æµ‹è¯•é¢„æµ‹å·¥ä½œæµç¨‹ ===")

        try:
            # åˆ›å»ºæµ‹è¯•æ•°æ®
            test_data = self.create_realistic_test_data(600)

            # åŠ è½½å·²è®­ç»ƒçš„æ¨¡å‹
            models_to_test = [
                (ModelType.LSTM, "lstm_model.pkl"),
                (ModelType.DLINEAR, "dlinear_model.pkl"),
                (ModelType.ENSEMBLE, "ensemble_model.pkl")
            ]

            prediction_results = {}

            for model_type, model_file in models_to_test:
                model_path = os.path.join(self.temp_dir, model_file)

                if os.path.exists(model_path):
                    logger.info(f"æµ‹è¯• {model_type.value} é¢„æµ‹...")

                    # åŠ è½½æ¨¡å‹
                    predictor = WorkloadPredictor(
                        model_type=model_type,
                        prediction_horizon=PredictionHorizon.SHORT_TERM,
                        device='cpu'
                    )
                    predictor.load(model_path)

                    # è¿›è¡Œé¢„æµ‹
                    historical_data = test_data.iloc[:-20]
                    prediction_result = predictor.predict(historical_data, steps=15)

                    # ç”ŸæˆæŠ¥å‘Š
                    report = predictor.generate_prediction_report(prediction_result, historical_data)

                    prediction_results[model_type.value] = {
                        'prediction_success': True,
                        'prediction_count': len(prediction_result.predicted_requests),
                        'confidence': prediction_result.confidence,
                        'report_sections': list(report.keys())
                    }

                    logger.info(f"{model_type.value} é¢„æµ‹å®Œæˆï¼Œç½®ä¿¡åº¦: {prediction_result.confidence:.3f}")
                else:
                    logger.warning(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
                    prediction_results[model_type.value] = {
                        'prediction_success': False,
                        'error': 'Model file not found'
                    }

            self.test_results['prediction'] = prediction_results

            # éªŒè¯é¢„æµ‹ç»“æœ
            successful_predictions = [r for r in prediction_results.values() if r['prediction_success']]
            assert len(successful_predictions) > 0, "åº”è¯¥æœ‰è‡³å°‘ä¸€ä¸ªæˆåŠŸçš„é¢„æµ‹"

            logger.info("âœ… é¢„æµ‹å·¥ä½œæµç¨‹æµ‹è¯•é€šè¿‡")
            return True

        except Exception as e:
            logger.error(f"âŒ é¢„æµ‹å·¥ä½œæµç¨‹æµ‹è¯•å¤±è´¥: {e}")
            traceback.print_exc()
            self.test_results['prediction'] = {'success': False, 'error': str(e)}
            return False

    def test_visualization_workflow(self) -> bool:
        """æµ‹è¯•å¯è§†åŒ–å·¥ä½œæµç¨‹"""
        logger.info("=== æµ‹è¯•å¯è§†åŒ–å·¥ä½œæµç¨‹ ===")

        try:
            # åˆ›å»ºæµ‹è¯•æ•°æ®
            test_data = self.create_realistic_test_data(400)

            # åŠ è½½é›†æˆæ¨¡å‹
            ensemble_model_path = os.path.join(self.temp_dir, "ensemble_model.pkl")

            if os.path.exists(ensemble_model_path):
                # åŠ è½½æ¨¡å‹
                predictor = WorkloadPredictor(
                    model_type=ModelType.ENSEMBLE,
                    prediction_horizon=PredictionHorizon.SHORT_TERM,
                    device='cpu'
                )
                predictor.load(ensemble_model_path)

                # è¿›è¡Œé¢„æµ‹
                historical_data = test_data.iloc[:-15]
                prediction_result = predictor.predict(historical_data, steps=10)

                # åˆ›å»ºå¯è§†åŒ–
                visualizer = SimplePredictionVisualizer()

                # æµ‹è¯•å•æ¨¡å‹å¯è§†åŒ–
                single_plot_path = os.path.join(self.temp_dir, "single_prediction.png")
                single_metrics = visualizer.plot_prediction_comparison(
                    historical_data, prediction_result, "Ensemble Model",
                    save_path=single_plot_path, show_plot=False
                )

                # æµ‹è¯•å¤šæ¨¡å‹å¯¹æ¯”
                multi_models_results = {}
                for model_type in [ModelType.LSTM, ModelType.DLINEAR]:
                    model_path = os.path.join(self.temp_dir, f"{model_type.value.lower()}_model.pkl")
                    if os.path.exists(model_path):
                        model_predictor = WorkloadPredictor(
                            model_type=model_type,
                            prediction_horizon=PredictionHorizon.SHORT_TERM,
                            device='cpu'
                        )
                        model_predictor.load(model_path)
                        model_prediction = model_predictor.predict(historical_data, steps=10)
                        multi_models_results[model_type.value] = model_prediction

                if multi_models_results:
                    multi_plot_path = os.path.join(self.temp_dir, "multi_model_comparison.png")
                    multi_metrics = visualizer.plot_multi_model_comparison(
                        multi_models_results, historical_data,
                        save_path=multi_plot_path, show_plot=False
                    )

                # ç”ŸæˆæŠ¥å‘Š
                report_path = os.path.join(self.temp_dir, "prediction_report.txt")
                report = visualizer.generate_simple_report(
                    {"Ensemble": prediction_result}, historical_data,
                    save_path=report_path
                )

                self.test_results['visualization'] = {
                    'success': True,
                    'single_plot_created': os.path.exists(single_plot_path),
                    'multi_plot_created': os.path.exists(multi_plot_path),
                    'report_created': os.path.exists(report_path),
                    'single_metrics': single_metrics,
                    'models_compared': len(multi_models_results)
                }

                logger.info("âœ… å¯è§†åŒ–å·¥ä½œæµç¨‹æµ‹è¯•é€šè¿‡")
                return True
            else:
                logger.warning("é›†æˆæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡å¯è§†åŒ–æµ‹è¯•")
                self.test_results['visualization'] = {
                    'success': False,
                    'error': 'Ensemble model not found'
                }
                return False

        except Exception as e:
            logger.error(f"âŒ å¯è§†åŒ–å·¥ä½œæµç¨‹æµ‹è¯•å¤±è´¥: {e}")
            traceback.print_exc()
            self.test_results['visualization'] = {'success': False, 'error': str(e)}
            return False

    def test_error_recovery_workflow(self) -> bool:
        """æµ‹è¯•é”™è¯¯æ¢å¤å·¥ä½œæµç¨‹"""
        logger.info("=== æµ‹è¯•é”™è¯¯æ¢å¤å·¥ä½œæµç¨‹ ===")

        try:
            # æµ‹è¯•1: æ•°æ®æ ¼å¼é”™è¯¯
            logger.info("æµ‹è¯•æ•°æ®æ ¼å¼é”™è¯¯å¤„ç†...")
            malformed_data = pd.DataFrame({
                'wrong_column': [1, 2, 3],
                'another_wrong': [4, 5, 6]
            })

            try:
                predictor = WorkloadPredictor(model_type=ModelType.LSTM, device='cpu')
                predictor.validate_data(malformed_data)
                validation_success = False
            except:
                validation_success = True  # åº”è¯¥æŠ›å‡ºå¼‚å¸¸

            # æµ‹è¯•2: æ¨¡å‹åŠ è½½é”™è¯¯
            logger.info("æµ‹è¯•æ¨¡å‹åŠ è½½é”™è¯¯å¤„ç†...")
            try:
                predictor = WorkloadPredictor(model_type=ModelType.LSTM, device='cpu')
                predictor.load("non_existent_model.pkl")
                loading_success = False
            except:
                loading_success = True  # åº”è¯¥æŠ›å‡ºå¼‚å¸¸

            # æµ‹è¯•3: é¢„æµ‹é”™è¯¯æ¢å¤
            logger.info("æµ‹è¯•é¢„æµ‹é”™è¯¯æ¢å¤...")
            test_data = self.create_realistic_test_data(50)  # æ•°æ®ä¸è¶³

            predictor = WorkloadPredictor(
                model_type=ModelType.LSTM,
                prediction_horizon=PredictionHorizon.SHORT_TERM,
                sequence_length=100,  # åºåˆ—è¿‡é•¿
                device='cpu'
            )

            # å³ä½¿è®­ç»ƒå¤±è´¥ï¼Œé¢„æµ‹ä¹Ÿåº”è¯¥æœ‰åˆç†çš„å¤„ç†
            try:
                prediction_result = predictor.predict(test_data, steps=10)
                recovery_success = prediction_result is not None
            except:
                recovery_success = False

            self.test_results['error_recovery'] = {
                'data_format_validation': validation_success,
                'model_loading_handling': loading_success,
                'prediction_recovery': recovery_success
            }

            logger.info("âœ… é”™è¯¯æ¢å¤å·¥ä½œæµç¨‹æµ‹è¯•é€šè¿‡")
            return True

        except Exception as e:
            logger.error(f"âŒ é”™è¯¯æ¢å¤å·¥ä½œæµç¨‹æµ‹è¯•å¤±è´¥: {e}")
            traceback.print_exc()
            self.test_results['error_recovery'] = {'success': False, 'error': str(e)}
            return False

    def run_all_tests(self) -> bool:
        """è¿è¡Œæ‰€æœ‰ç«¯åˆ°ç«¯æµ‹è¯•"""
        logger.info("å¼€å§‹ç«¯åˆ°ç«¯é›†æˆæµ‹è¯•...")

        test_functions = [
            self.test_data_preprocessing_workflow,
            self.test_model_training_workflow,
            self.test_prediction_workflow,
            self.test_visualization_workflow,
            self.test_error_recovery_workflow
        ]

        passed_tests = 0
        total_tests = len(test_functions)

        try:
            for test_func in test_functions:
                test_name = test_func.__name__.replace('test_', '').replace('_', ' ').title()
                logger.info(f"\n{'='*60}")
                logger.info(f"å¼€å§‹æµ‹è¯•: {test_name}")
                logger.info(f"{'='*60}")

                try:
                    if test_func():
                        logger.info(f"âœ… {test_name} æµ‹è¯•é€šè¿‡")
                        passed_tests += 1
                    else:
                        logger.error(f"âŒ {test_name} æµ‹è¯•å¤±è´¥")
                except Exception as e:
                    logger.error(f"âŒ {test_name} æµ‹è¯•å´©æºƒ: {e}")
                    traceback.print_exc()

            logger.info(f"\n{'='*60}")
            logger.info("ç«¯åˆ°ç«¯æµ‹è¯•æ€»ç»“")
            logger.info(f"{'='*60}")
            logger.info(f"é€šè¿‡æµ‹è¯•: {passed_tests}/{total_tests}")
            logger.info(f"é€šè¿‡ç‡: {passed_tests/total_tests*100:.1f}%")

            # ä¿å­˜æµ‹è¯•ç»“æœ
            results_path = os.path.join(self.temp_dir, "e2e_test_results.json")
            with open(results_path, 'w', encoding='utf-8') as f:
                json.dump(self.test_results, f, indent=2, ensure_ascii=False)

            logger.info(f"æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {results_path}")

            if passed_tests == total_tests:
                logger.info("ğŸ‰ æ‰€æœ‰çš„ç«¯åˆ°ç«¯é›†æˆæµ‹è¯•é€šè¿‡ï¼")
                return True
            else:
                logger.error("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥")
                return False

        except Exception as e:
            logger.error(f"âŒ æµ‹è¯•è¿‡ç¨‹å‡ºé”™: {e}")
            traceback.print_exc()
            return False

        finally:
            self.cleanup()


def main():
    """ä¸»å‡½æ•°"""
    test_runner = EndToEndTestRunner()
    success = test_runner.run_all_tests()
    return success


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)